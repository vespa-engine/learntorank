[
  {
    "objectID": "module_stats.html",
    "href": "module_stats.html",
    "title": "stats",
    "section": "",
    "text": "source\n\n\n\n bootstrap_sampling (data:pandas.core.frame.DataFrame,\n                     estimator:Callable=<function mean>, n_boot:int=1000,\n                     columns_to_exclude:List[str]=None)\n\nCompute bootstrap estimates of the data distribution\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nDataFrame\n\nData containing the columns we want to generate bootstrap estimates from.\n\n\nestimator\ntyping.Callable\nmean\nestimator function that accepts an array-like argument.\n\n\nn_boot\nint\n1000\nNumber of bootstrap estimates to compute.\n\n\ncolumns_to_exclude\ntyping.List[str]\nNone\nColumn names to exclude.\n\n\n\nUsage:\nGenerate data with columns containing data that we want to compute estimates from. The values in the column a comes from Normal distribution with mean 0 and standard deviation 1. The values from column b comes from Normal distribution with mean 100 and standard deviation 10.\n\ndata = pd.DataFrame(\n    data={\n        \"a\": np.random.normal(size = 100), \n        \"b\": np.random.normal(loc=100, scale = 10, size = 100)\n    }\n)\ndata.head()\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      0.605639\n      92.817505\n    \n    \n      1\n      -0.775791\n      92.750026\n    \n    \n      2\n      -1.265231\n      107.981771\n    \n    \n      3\n      0.981306\n      101.388385\n    \n    \n      4\n      0.029075\n      122.700172\n    \n  \n\n\n\n\n\n\nBy default, the function generates the mean of each column n_boot times. Each value represents the mean obtained from a bootstrap sample of the original data.\n\nestimates = bootstrap_sampling(data, n_boot=100)\nestimates\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      0.012356\n      100.018394\n    \n    \n      1\n      0.143189\n      100.691872\n    \n    \n      2\n      -0.002554\n      99.874399\n    \n    \n      3\n      0.079395\n      99.539636\n    \n    \n      4\n      0.055096\n      100.452383\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      95\n      0.063409\n      100.439363\n    \n    \n      96\n      -0.024455\n      98.607045\n    \n    \n      97\n      0.209427\n      99.866736\n    \n    \n      98\n      0.061323\n      98.680469\n    \n    \n      99\n      0.289456\n      99.980295\n    \n  \n\n100 rows × 2 columns\n\n\n\nWe can check if the estimates make sense by compute the mean of the bootstrap estimates and comparing with the mean of the Normal distribution they were generated from.\n\nestimates.mean()\n\na      0.089538\nb    100.099900\ndtype: float64\n\n\n\n\n\nWe can specify other functions, such as np.std to compute the standard deviation.\n\nestimates = bootstrap_sampling(data, estimator=np.std, n_boot=100)\nestimates\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      0.933496\n      10.126658\n    \n    \n      1\n      0.929125\n      9.852667\n    \n    \n      2\n      0.899762\n      10.307814\n    \n    \n      3\n      0.968039\n      10.416074\n    \n    \n      4\n      1.004349\n      10.441463\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      95\n      0.910904\n      10.357727\n    \n    \n      96\n      0.818276\n      12.358640\n    \n    \n      97\n      0.981826\n      9.622724\n    \n    \n      98\n      0.962237\n      10.897055\n    \n    \n      99\n      0.913994\n      11.096338\n    \n  \n\n100 rows × 2 columns\n\n\n\nIf we take the mean of the bootstrap estimates of the standard deviation, we should recover a value close to the standard deviation of the distribution that the data were generated from.\n\nestimates.mean()\n\na     0.943942\nb    10.480457\ndtype: float64\n\n\n\n\n\n\nestimates = bootstrap_sampling(\n    data, n_boot=100, columns_to_exclude=[\"b\"]\n)\nestimates\n\n\n\n\n\n  \n    \n      \n      a\n    \n  \n  \n    \n      0\n      0.259128\n    \n    \n      1\n      0.098232\n    \n    \n      2\n      0.087111\n    \n    \n      3\n      -0.131376\n    \n    \n      4\n      0.050997\n    \n    \n      ...\n      ...\n    \n    \n      95\n      0.129835\n    \n    \n      96\n      -0.004873\n    \n    \n      97\n      -0.046338\n    \n    \n      98\n      0.246239\n    \n    \n      99\n      0.355848\n    \n  \n\n100 rows × 1 columns\n\n\n\n\nsource\n\n\n\n\n\n compute_evaluation_estimates (df:pandas.core.frame.DataFrame,\n                               n_boot:int=1000,\n                               estimator:Callable=<function mean>,\n                               quantile_low:float=0.025,\n                               quantile_high=0.975)\n\nCompute estimate and confidence interval for evaluation per query metrics.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\nDataFrame\n\nEvaluations per query data, usually obtained pyvespa evaluate method.\n\n\nn_boot\nint\n1000\nNumber of bootstrap samples.\n\n\nestimator\ntyping.Callable\nmean\nestimator function that accepts an array-like argument.\n\n\nquantile_low\nfloat\n0.025\nlower quantile to compute confidence interval\n\n\nquantile_high\nfloat\n0.975\nupper quantile to compute confidence interval\n\n\n\nUsage:\nGenerate sample data frame, which must contain the column model.\n\nnumber_data_points = 1000\ndata = pd.DataFrame(\n    data = {\n        \"model\": (\n            [\"A\"] * number_data_points + \n            [\"B\"] * number_data_points\n        ),\n        \"query_id\": (\n            list(range(number_data_points)) + \n            list(range(number_data_points))\n        ),\n        \"metric_1\": (\n            np.random.binomial(size=number_data_points, n=1, p=0.3).tolist() + \n            np.random.binomial(size=number_data_points, n=1, p=0.7).tolist()\n        ),\n        \"metric_2\": (\n            np.random.binomial(size=number_data_points, n=1, p=0.1).tolist() + \n            np.random.binomial(size=number_data_points, n=1, p=0.9).tolist()\n        )\n        \n    }\n).sort_values(\"query_id\").reset_index(drop=True)\ndata\n\n\n\n\n\n  \n    \n      \n      model\n      query_id\n      metric_1\n      metric_2\n    \n  \n  \n    \n      0\n      A\n      0\n      0\n      0\n    \n    \n      1\n      B\n      0\n      1\n      1\n    \n    \n      2\n      A\n      1\n      0\n      1\n    \n    \n      3\n      B\n      1\n      1\n      1\n    \n    \n      4\n      A\n      2\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1995\n      A\n      997\n      1\n      0\n    \n    \n      1996\n      B\n      998\n      1\n      1\n    \n    \n      1997\n      A\n      998\n      1\n      0\n    \n    \n      1998\n      A\n      999\n      0\n      0\n    \n    \n      1999\n      B\n      999\n      0\n      1\n    \n  \n\n2000 rows × 4 columns\n\n\n\n\n\n\ncompute_evaluation_estimates(data)\n\n\n\n\n\n  \n    \n      \n      metric\n      model\n      low\n      median\n      high\n    \n  \n  \n    \n      0\n      metric_1\n      A\n      0.268000\n      0.296\n      0.325\n    \n    \n      1\n      metric_1\n      B\n      0.667000\n      0.696\n      0.724\n    \n    \n      2\n      metric_2\n      A\n      0.091000\n      0.109\n      0.129\n    \n    \n      3\n      metric_2\n      B\n      0.887975\n      0.907\n      0.924\n    \n  \n\n\n\n\n\n\n\n\ncompute_evaluation_estimates(data, estimator=np.std)\n\n\n\n\n\n  \n    \n      \n      metric\n      model\n      low\n      median\n      high\n    \n  \n  \n    \n      0\n      metric_1\n      A\n      0.442918\n      0.456491\n      0.468375\n    \n    \n      1\n      metric_1\n      B\n      0.448001\n      0.459983\n      0.470931\n    \n    \n      2\n      metric_2\n      A\n      0.289026\n      0.311639\n      0.335200\n    \n    \n      3\n      metric_2\n      B\n      0.264998\n      0.291829\n      0.315366\n    \n  \n\n\n\n\n\n\n\n\ncompute_evaluation_estimates(\n    data, \n    quantile_low=0.2, \n    quantile_high=0.8\n)\n\n\n\n\n\n  \n    \n      \n      metric\n      model\n      low\n      median\n      high\n    \n  \n  \n    \n      0\n      metric_1\n      A\n      0.285\n      0.296\n      0.308\n    \n    \n      1\n      metric_1\n      B\n      0.684\n      0.696\n      0.708\n    \n    \n      2\n      metric_2\n      A\n      0.102\n      0.110\n      0.118\n    \n    \n      3\n      metric_2\n      B\n      0.898\n      0.906\n      0.914\n    \n  \n\n\n\n\n\ncompute_evaluation_estimates(data[[\"model\", \"metric_1\", \"metric_2\"]])\n\n\n\n\n\n  \n    \n      \n      metric\n      model\n      low\n      median\n      high\n    \n  \n  \n    \n      0\n      metric_1\n      A\n      0.269975\n      0.297\n      0.326000\n    \n    \n      1\n      metric_1\n      B\n      0.667975\n      0.696\n      0.726000\n    \n    \n      2\n      metric_2\n      A\n      0.091000\n      0.109\n      0.129025\n    \n    \n      3\n      metric_2\n      B\n      0.888000\n      0.907\n      0.923000"
  },
  {
    "objectID": "module_ranking.html",
    "href": "module_ranking.html",
    "title": "ranking",
    "section": "",
    "text": "source\n\nkeras_linear_model\n\n keras_linear_model (number_documents_per_query, number_features)\n\nlinear model with a lasso constrain on the kernel weights.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nnumber_documents_per_query\n\nNumber of documents per query to reshape the listwise prediction.\n\n\nnumber_features\n\nNumber of features used per document.\n\n\nReturns\nSequential\nThe uncompiled Keras model.\n\n\n\nUsage:\n\nklm = keras_linear_model(\n    number_documents_per_query=10, \n    number_features=5\n)\n\n\nsource\n\n\nkeras_lasso_linear_model\n\n keras_lasso_linear_model (number_documents_per_query, number_features,\n                           l1_penalty, normalization_layer:Optional=None)\n\nlinear model with a lasso constrain on the kernel weights.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnumber_documents_per_query\n\n\nNumber of documents per query to reshape the listwise prediction.\n\n\nnumber_features\n\n\nNumber of features used per document.\n\n\nl1_penalty\n\n\nControls the L1-norm penalty.\n\n\nnormalization_layer\ntyping.Optional\nNone\nInitialized normalization layers. Used when performing feature selection.\n\n\nReturns\nSequential\n\nThe uncompiled Keras model.\n\n\n\nUsage:\n\nkllm = keras_lasso_linear_model(\n    number_documents_per_query=10, \n    number_features=5, \n    l1_penalty=0.01\n)\n\n\nsource\n\n\nkeras_ndcg_compiled_model\n\n keras_ndcg_compiled_model (model, learning_rate, top_n)\n\nCompile listwise Keras model with NDCG stateless metric and ApproxNDCGLoss\n\n\n\n\nDetails\n\n\n\n\nmodel\nUncompiled Keras model\n\n\nlearning_rate\nLearning rate used in the Adagrad optim algo.\n\n\ntop_n\nTop n used when computing the NDCG metric\n\n\n\nUsage:\n\ncompiled_klm = keras_ndcg_compiled_model(\n    model=klm, \n    learning_rate=0.1, \n    top_n=10\n)\n\n\nsource\n\n\nLinearHyperModel\n\n LinearHyperModel (number_documents_per_query, number_features, top_n=10,\n                   learning_rate_range=None)\n\nDefine a KerasTuner search space for linear models\n\nlinear_hyper_model = LinearHyperModel(\n    number_documents_per_query=10, \n    number_features=10, \n    top_n=10, \n    learning_rate_range=[1e-2, 1e2]\n)\n\n\nsource\n\n\nLassoHyperModel\n\n LassoHyperModel (number_documents_per_query, number_features,\n                  trained_normalization_layer, top_n=10,\n                  l1_penalty_range=None, learning_rate_range=None)\n\nDefine a KerasTuner search space for lasso models\n\nsource\n\n\nListwiseRankingFramework\n\n ListwiseRankingFramework (number_documents_per_query, batch_size=32,\n                           shuffle_buffer_size=1000, tuner_max_trials=3,\n                           tuner_executions_per_trial=1, tuner_epochs=1,\n                           tuner_early_stop_patience=None, final_epochs=1,\n                           top_n=10, l1_penalty_range=None,\n                           learning_rate_range=None, folder_dir='/home/run\n                           ner/work/learntorank/learntorank')\n\nListwise ranking framework"
  },
  {
    "objectID": "stateless_sequence_classification_task.html",
    "href": "stateless_sequence_classification_task.html",
    "title": "Sequence Classification task",
    "section": "",
    "text": "Vespa has implemented accelerated model evaluation using ONNX Runtime in the stateless cluster. This opens up new usage areas for Vespa, such as serving model predictions."
  },
  {
    "objectID": "stateless_sequence_classification_task.html#define-the-model-server",
    "href": "stateless_sequence_classification_task.html#define-the-model-server",
    "title": "Sequence Classification task",
    "section": "Define the model server",
    "text": "Define the model server\nThe SequenceClassification task takes a text input and returns an array of floats that depends on the model used to solve the task. The model argument can be the id of the model as defined by the huggingface model hub.\n\nfrom learntorank.ml import SequenceClassification\n\ntask = SequenceClassification(\n    model_id=\"bert_tiny\", \n    model=\"google/bert_uncased_L-2_H-128_A-2\"\n)\n\nA ModelServer is a simplified application package focused on stateless model evaluation. It can take as many tasks as we want.\n\nfrom vespa.package import ModelServer\n\nmodel_server = ModelServer(\n    name=\"bertModelServer\",\n    tasks=[task],\n)"
  },
  {
    "objectID": "stateless_sequence_classification_task.html#deploy-the-model-server",
    "href": "stateless_sequence_classification_task.html#deploy-the-model-server",
    "title": "Sequence Classification task",
    "section": "Deploy the model server",
    "text": "Deploy the model server\nWe can either host our model server on Vespa Cloud or deploy it locally using a Docker container.\n\nfrom vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker()\napp = vespa_docker.deploy(application_package=model_server)\n\nUsing framework PyTorch: 1.12.1\nFound input input_ids with shape: {0: 'batch', 1: 'sequence'}\nFound input token_type_ids with shape: {0: 'batch', 1: 'sequence'}\nFound input attention_mask with shape: {0: 'batch', 1: 'sequence'}\nFound output output_0 with shape: {0: 'batch'}\nEnsuring inputs are in correct order\nposition_ids is not present in the generated input list.\nGenerated inputs order: ['input_ids', 'attention_mask', 'token_type_ids']\nWaiting for configuration server, 0/300 seconds...\nWaiting for configuration server, 5/300 seconds...\nWaiting for application status, 0/300 seconds...\nWaiting for application status, 5/300 seconds...\nFinished deployment."
  },
  {
    "objectID": "stateless_sequence_classification_task.html#get-model-information",
    "href": "stateless_sequence_classification_task.html#get-model-information",
    "title": "Sequence Classification task",
    "section": "Get model information",
    "text": "Get model information\nGet models available:\n\napp.get_model_endpoint()\n\n{'bert_tiny': 'http://localhost:8080/model-evaluation/v1/bert_tiny'}\n\n\nGet information about a specific model:\n\napp.get_model_endpoint(model_id=\"bert_tiny\")\n\n{'model': 'bert_tiny',\n 'functions': [{'function': 'output_0',\n   'info': 'http://localhost:8080/model-evaluation/v1/bert_tiny/output_0',\n   'eval': 'http://localhost:8080/model-evaluation/v1/bert_tiny/output_0/eval',\n   'arguments': [{'name': 'input_ids', 'type': 'tensor(d0[],d1[])'},\n    {'name': 'attention_mask', 'type': 'tensor(d0[],d1[])'},\n    {'name': 'token_type_ids', 'type': 'tensor(d0[],d1[])'}]}]}"
  },
  {
    "objectID": "stateless_sequence_classification_task.html#get-predictions",
    "href": "stateless_sequence_classification_task.html#get-predictions",
    "title": "Sequence Classification task",
    "section": "Get predictions",
    "text": "Get predictions\nGet a prediction:\n\napp.predict(x=\"this is a test\", model_id=\"bert_tiny\")\n\n[-0.00954509899020195, 0.2504960000514984]"
  },
  {
    "objectID": "stateless_sequence_classification_task.html#cleanup",
    "href": "stateless_sequence_classification_task.html#cleanup",
    "title": "Sequence Classification task",
    "section": "Cleanup",
    "text": "Cleanup\n\nfrom shutil import rmtree\n\nvespa_docker.container.stop(timeout=600)\nvespa_docker.container.remove()"
  },
  {
    "objectID": "passage_uncertainty_evaluation.html",
    "href": "passage_uncertainty_evaluation.html",
    "title": "IR evaluation metrics with uncertainty estimates",
    "section": "",
    "text": "When working with search engine apps, be it a text search or a recommendation system, part of the job is doing experiments around components such as ranking functions and deciding which experiments deliver the best result.\nThis tutorial builds a text search app with Vespa, feeds a sample of the passage ranking dataset to the app, and evaluates two ranking functions across three different metrics. In addition to return point estimates of the evaluation metrics, we compute confidence intervals as illustrated in the plot below. Measuring uncertainty around the metric estimates gives us a better sense of how significant is the impact of our changes in the application.\nThe code and the data used in this end-to-end tutorial are available and can be reproduced in a Jupyter Notebook."
  },
  {
    "objectID": "passage_uncertainty_evaluation.html#create-the-vespa-application-package",
    "href": "passage_uncertainty_evaluation.html#create-the-vespa-application-package",
    "title": "IR evaluation metrics with uncertainty estimates",
    "section": "Create the Vespa application package",
    "text": "Create the Vespa application package\nCreate a Vespa application package to perform passage ranking experiments using the create_basic_search_package.\n\nfrom learntorank.passage import create_basic_search_package\n\napp_package = create_basic_search_package()\n\nWe can inspect how the Vespa search definition file looks like:\n\nprint(app_package.schema.schema_to_text)\n\nschema PassageRanking {\n    document PassageRanking {\n        field doc_id type string {\n            indexing: attribute | summary\n        }\n        field text type string {\n            indexing: index | summary\n            index: enable-bm25\n        }\n    }\n    fieldset default {\n        fields: text\n    }\n    rank-profile bm25 {\n        first-phase {\n            expression: bm25(text)\n        }\n        summary-features {\n            bm25(text)\n        }\n    }\n    rank-profile native_rank {\n        first-phase {\n            expression: nativeRank(text)\n        }\n    }\n}\n\n\nIn this tutorial, we are going to compare two ranking functions. One is based on NativeRank, and the other is based on BM25."
  },
  {
    "objectID": "passage_uncertainty_evaluation.html#deploy-the-application",
    "href": "passage_uncertainty_evaluation.html#deploy-the-application",
    "title": "IR evaluation metrics with uncertainty estimates",
    "section": "Deploy the application",
    "text": "Deploy the application\nDeploy the application package in a Docker container for local development. Alternatively, it is possible to deploy the application package to Vespa Cloud.\n\nfrom vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker()\napp = vespa_docker.deploy(application_package=app_package)\n\nWaiting for configuration server, 0/300 seconds...\nWaiting for configuration server, 5/300 seconds...\nWaiting for application status, 0/300 seconds...\nWaiting for application status, 5/300 seconds...\nWaiting for application status, 10/300 seconds...\nWaiting for application status, 15/300 seconds...\nWaiting for application status, 20/300 seconds...\nFinished deployment.\n\n\nOnce the deployment is finished, we can interact with the deployed application through the app variable."
  },
  {
    "objectID": "passage_uncertainty_evaluation.html#get-sample-data",
    "href": "passage_uncertainty_evaluation.html#get-sample-data",
    "title": "IR evaluation metrics with uncertainty estimates",
    "section": "Get sample data",
    "text": "Get sample data\nWe can load passage ranking sample data with PassageData.load. By default, it will download pre-generated sample data.\n\nfrom learntorank.passage import PassageData\n\ndata = PassageData.load()\n\n\ndata\n\nPassageData(corpus, train_qrels, train_queries, dev_qrels, dev_queries)\n\n\n\ndata.summary\n\nNumber of documents: 1000\nNumber of train queries: 100\nNumber of train relevance judgments: 100\nNumber of dev queries: 100\nNumber of dev relevance judgments: 100"
  },
  {
    "objectID": "passage_uncertainty_evaluation.html#feed-the-application",
    "href": "passage_uncertainty_evaluation.html#feed-the-application",
    "title": "IR evaluation metrics with uncertainty estimates",
    "section": "Feed the application",
    "text": "Feed the application\nGet the document corpus in a DataFrame format.\n\ncorpus_df = data.get_corpus()\ncorpus_df.head()\n\n\n\n\n\n  \n    \n      \n      doc_id\n      text\n    \n  \n  \n    \n      0\n      5954248\n      Why GameStop is excited for Dragon Age: Inquis...\n    \n    \n      1\n      7290700\n      metaplasia definition: 1. abnormal change of o...\n    \n    \n      2\n      5465518\n      Candice Net Worth. According to the report of ...\n    \n    \n      3\n      3100518\n      Under the Base Closure Act, March AFB was down...\n    \n    \n      4\n      3207764\n      There are a number of career opportunities for...\n    \n  \n\n\n\n\nFeed the data to the deployed application.\n\nresponses = app.feed_df(df=corpus_df, include_id=True, id_field=\"doc_id\")\n\nSuccessful documents fed: 1000/1000.\nBatch progress: 1/1.\n\n\nWe can also check the number of successfully fed documents through the responses status code:\n\nsum([response.status_code == 200 for response in responses])\n\n1000"
  },
  {
    "objectID": "passage_uncertainty_evaluation.html#query-the-application",
    "href": "passage_uncertainty_evaluation.html#query-the-application",
    "title": "IR evaluation metrics with uncertainty estimates",
    "section": "Query the application",
    "text": "Query the application\nGet the dev set queries in a DataFrame format.\n\ndev_queries_df = data.get_queries(type=\"dev\")\ndev_queries_df.head()\n\n\n\n\n\n  \n    \n      \n      query_id\n      query\n    \n  \n  \n    \n      0\n      1101971\n      why say the sky is the limit\n    \n    \n      1\n      712898\n      what is an cvc in radiology\n    \n    \n      2\n      154469\n      dmv california how long does it take to get id\n    \n    \n      3\n      930015\n      what's an epigraph\n    \n    \n      4\n      860085\n      what is va tax\n    \n  \n\n\n\n\nGet the first query text to use as an example when querying our passage search application.\n\nsample_query = dev_queries_df.loc[0, \"query\"]\nsample_query\n\n'why say the sky is the limit'\n\n\n\nQuery with QueryModel\nCreate the bm25 QueryModel, which uses Vespa’s weakAnd operator to match documents relevant to the query and use the bm25 rank-profile that we defined in the application package above to rank the documents.\n\nfrom learntorank.query import QueryModel, WeakAnd, Ranking\n\nbm25_query_model = QueryModel(\n    name=\"bm25\", \n    match_phase=WeakAnd(hits=100), \n    ranking=Ranking(name=\"bm25\")\n)\n\nOnce a QueryModel is specified, we can use it to query our application.\n\nfrom learntorank.query import send_query\nfrom pprint import pprint\n\nresponse = send_query(\n    app=app,\n    query=sample_query, \n    query_model=bm25_query_model\n)\npprint(response.hits[0:2])\n\n[{'fields': {'doc_id': '7407715',\n             'documentid': 'id:PassageRanking:PassageRanking::7407715',\n             'sddocname': 'PassageRanking',\n             'summaryfeatures': {'bm25(text)': 11.979235042476953,\n                                 'vespa.summaryFeatures.cached': 0.0},\n             'text': 'The Sky is the Limit also known as TSITL is a global '\n                     'effort designed to influence, motivate and inspire '\n                     'people all over the world to achieve their goals and '\n                     'dreams in life. TSITL’s collaborative community on '\n                     'social media provides you with a vast archive of '\n                     'motivational pictures/quotes/videos.'},\n  'id': 'id:PassageRanking:PassageRanking::7407715',\n  'relevance': 11.979235042476953,\n  'source': 'PassageRanking_content'},\n {'fields': {'doc_id': '84721',\n             'documentid': 'id:PassageRanking:PassageRanking::84721',\n             'sddocname': 'PassageRanking',\n             'summaryfeatures': {'bm25(text)': 11.310323797415357,\n                                 'vespa.summaryFeatures.cached': 0.0},\n             'text': 'Sky Customer Service 0870 280 2564. Use the Sky contact '\n                     'number to get in contact with the Sky customer services '\n                     'team to speak to a representative about your Sky TV, Sky '\n                     'Internet or Sky telephone services. The Sky customer '\n                     'Services team is operational between 8:30am and 11:30pm '\n                     'seven days a week.'},\n  'id': 'id:PassageRanking:PassageRanking::84721',\n  'relevance': 11.310323797415357,\n  'source': 'PassageRanking_content'}]\n\n\n\n\nQuery with Vespa Query Language\nWe can also translate the query created with the QueryModel into the Vespa Query Language (YQL) by setting debug_request=True:\n\nresponse = send_query(\n    app=app,\n    query = sample_query, \n    query_model=bm25_query_model, \n    debug_request=True\n)\nyql_body = response.request_body\npprint(yql_body)\n\n{'ranking': {'listFeatures': 'false', 'profile': 'bm25'},\n 'yql': 'select * from sources * where ({targetHits: 100}weakAnd(default '\n        'contains \"why\", default contains \"say\", default contains \"the\", '\n        'default contains \"sky\", default contains \"is\", default contains '\n        '\"the\", default contains \"limit\"));'}\n\n\nWe can use Vespa YQL directly via the body parameter:\n\nyql_response = send_query(app=app, body=yql_body)\npprint(yql_response.hits[0:2])\n\n[{'fields': {'doc_id': '7407715',\n             'documentid': 'id:PassageRanking:PassageRanking::7407715',\n             'sddocname': 'PassageRanking',\n             'summaryfeatures': {'bm25(text)': 11.979235042476953,\n                                 'vespa.summaryFeatures.cached': 0.0},\n             'text': 'The Sky is the Limit also known as TSITL is a global '\n                     'effort designed to influence, motivate and inspire '\n                     'people all over the world to achieve their goals and '\n                     'dreams in life. TSITL’s collaborative community on '\n                     'social media provides you with a vast archive of '\n                     'motivational pictures/quotes/videos.'},\n  'id': 'id:PassageRanking:PassageRanking::7407715',\n  'relevance': 11.979235042476953,\n  'source': 'PassageRanking_content'},\n {'fields': {'doc_id': '84721',\n             'documentid': 'id:PassageRanking:PassageRanking::84721',\n             'sddocname': 'PassageRanking',\n             'summaryfeatures': {'bm25(text)': 11.310323797415357,\n                                 'vespa.summaryFeatures.cached': 0.0},\n             'text': 'Sky Customer Service 0870 280 2564. Use the Sky contact '\n                     'number to get in contact with the Sky customer services '\n                     'team to speak to a representative about your Sky TV, Sky '\n                     'Internet or Sky telephone services. The Sky customer '\n                     'Services team is operational between 8:30am and 11:30pm '\n                     'seven days a week.'},\n  'id': 'id:PassageRanking:PassageRanking::84721',\n  'relevance': 11.310323797415357,\n  'source': 'PassageRanking_content'}]"
  },
  {
    "objectID": "passage_uncertainty_evaluation.html#evaluate-query-models",
    "href": "passage_uncertainty_evaluation.html#evaluate-query-models",
    "title": "IR evaluation metrics with uncertainty estimates",
    "section": "Evaluate query models",
    "text": "Evaluate query models\nIn this section, we want to evaluate and compare the bm25_query_model defined above with the native_query_model defined below:\n\nnative_query_model = QueryModel(\n    name=\"native_rank\", \n    match_phase=WeakAnd(hits=100), \n    ranking=Ranking(name=\"native_rank\")\n)\n\nWe specify three metrics to evaluate the models.\n\nfrom learntorank.evaluation import (\n    Recall, \n    ReciprocalRank, \n    NormalizedDiscountedCumulativeGain\n)\n\nmetrics = [\n    Recall(at=10), \n    ReciprocalRank(at=3), \n    NormalizedDiscountedCumulativeGain(at=3)\n]\n\n\nPoint estimates\nIt is straightforward to obtain point estimates of the evaluation metrics for each query model being compared. In this case, we computed the mean and the standard deviation for each of the metrics.\n\nfrom learntorank.evaluation import evaluate\n\nevaluation = evaluate(\n    app=app,\n    labeled_data=data.get_labels(type=\"dev\"), \n    eval_metrics=metrics, \n    query_model=[native_query_model, bm25_query_model], \n    id_field=\"doc_id\",\n    aggregators=[\"mean\", \"std\"]\n )\n\n\nevaluation\n\n\n\n\n\n  \n    \n      \n      model\n      bm25\n      native_rank\n    \n  \n  \n    \n      recall_10\n      mean\n      0.935833\n      0.845833\n    \n    \n      std\n      0.215444\n      0.342749\n    \n    \n      reciprocal_rank_3\n      mean\n      0.935000\n      0.755000\n    \n    \n      std\n      0.231977\n      0.394587\n    \n    \n      ndcg_3\n      mean\n      0.912839\n      0.749504\n    \n    \n      std\n      0.242272\n      0.381792\n    \n  \n\n\n\n\nGiven the nature of the data distribution of the metrics described above, it is not trivial to compute a confidence interval from the mean and the standard deviation computed above. In the next section, we solve this by using bootstrap sampling on a per query metric evaluation.\n\n\nUncertainty estimates\nInstead of returning aggregated point estimates, we can also compute the metrics per query by setting per_query=True. This gives us more granular information on the distribution function of the metrics.\n\nevaluation_per_query = evaluate(\n    app=app,\n    labeled_data=data.get_labels(type=\"dev\"), \n    eval_metrics=metrics, \n    query_model=[native_query_model, bm25_query_model], \n    id_field=\"doc_id\",\n    per_query=True\n)\n\n\nevaluation_per_query.head()\n\n\n\n\n\n  \n    \n      \n      model\n      query_id\n      recall_10\n      reciprocal_rank_3\n      ndcg_3\n    \n  \n  \n    \n      0\n      native_rank\n      1101971\n      1.0\n      1.0\n      1.0\n    \n    \n      1\n      native_rank\n      712898\n      0.0\n      0.0\n      0.0\n    \n    \n      2\n      native_rank\n      154469\n      1.0\n      0.0\n      0.0\n    \n    \n      3\n      native_rank\n      930015\n      1.0\n      1.0\n      1.0\n    \n    \n      4\n      native_rank\n      860085\n      0.0\n      0.0\n      0.0\n    \n  \n\n\n\n\nWe then created a function that uses the evaluation per query data and computes uncertainty estimates via bootstrap sampling.\n\nfrom learntorank.stats import compute_evaluation_estimates\n\nestimates = compute_evaluation_estimates(\n    df = evaluation_per_query\n)\n\n\nestimates\n\n\n\n\n\n  \n    \n      \n      metric\n      model\n      low\n      median\n      high\n    \n  \n  \n    \n      0\n      ndcg_3\n      bm25\n      0.865279\n      0.914863\n      0.958766\n    \n    \n      1\n      ndcg_3\n      native_rank\n      0.675220\n      0.752400\n      0.820318\n    \n    \n      2\n      recall_10\n      bm25\n      0.891667\n      0.935833\n      0.974187\n    \n    \n      3\n      recall_10\n      native_rank\n      0.778312\n      0.847917\n      0.910000\n    \n    \n      4\n      reciprocal_rank_3\n      bm25\n      0.890000\n      0.935000\n      0.975000\n    \n    \n      5\n      reciprocal_rank_3\n      native_rank\n      0.678333\n      0.758333\n      0.831667\n    \n  \n\n\n\n\nWe can then create plots based on this data to make it easier to judge the magnitude of the differences between ranking functions.\n\nfrom plotnine import *\n\nprint((ggplot(estimates) + \n geom_point(aes(\"model\", \"median\")) + \n geom_errorbar(aes(x=\"model\", ymin=\"low\",ymax=\"high\")) + \n facet_wrap(\"metric\") + labs(y=\"Metric value\")\n))"
  },
  {
    "objectID": "passage_uncertainty_evaluation.html#cleanup-the-environment",
    "href": "passage_uncertainty_evaluation.html#cleanup-the-environment",
    "title": "IR evaluation metrics with uncertainty estimates",
    "section": "Cleanup the environment",
    "text": "Cleanup the environment\n\nvespa_docker.container.stop(timeout=600)\nvespa_docker.container.remove()"
  },
  {
    "objectID": "module_query.html",
    "href": "module_query.html",
    "title": "query",
    "section": "",
    "text": "source\n\n\n\n MatchFilter ()\n\nAbstract class for match filters.\n\nsource\n\n\n\n\n AND ()\n\nFilter that match document containing all the query terms.\nUsage: The AND filter is usually used when specifying query models.\n\nand_filter = AND()\n\n\nsource\n\n\n\n\n OR ()\n\nFilter that match any document containing at least one query term.\nUsage: The OR filter is usually used when specifying query models.\n\nor_filter = OR()\n\n\nsource\n\n\n\n\n WeakAnd (hits:int, field:str='default')\n\nMatch documents according to the weakAND algorithm.\nReference: https://docs.vespa.ai/en/using-wand-with-vespa.html\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nhits\nint\n\nLower bound on the number of hits to be retrieved.\n\n\nfield\nstr\ndefault\nWhich Vespa field to search.\n\n\nReturns\nNone\n\n\n\n\n\nUsage: The WeakAnd filter is usually used when specifying query models.\n\nweakand_filter = WeakAnd(hits=10, field=\"default\")\n\n\nsource\n\n\n\n\n Tokenize (hits:int, field:str='default')\n\nMatch documents according to the weakAND algorithm without parsing specials characters.\nReference: https://docs.vespa.ai/en/reference/simple-query-language-reference.html\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nhits\nint\n\nLower bound on the number of hits to be retrieved.\n\n\nfield\nstr\ndefault\nWhich Vespa field to search.\n\n\nReturns\nNone\n\n\n\n\n\nUsage: The Tokenize filter is usually used when specifying query models.\n\ntokenize_filter = Tokenize(hits=10, field=\"default\")\n\n\nsource\n\n\n\n\n ANN (doc_vector:str, query_vector:str, hits:int, label:str,\n      approximate:bool=True)\n\nMatch documents according to the nearest neighbor operator.\nReference: https://docs.vespa.ai/en/reference/query-language-reference.html\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndoc_vector\nstr\n\nName of the document field to be used in the distance calculation.\n\n\nquery_vector\nstr\n\nName of the query field to be used in the distance calculation.\n\n\nhits\nint\n\nLower bound on the number of hits to return.\n\n\nlabel\nstr\n\nA label to identify this specific operator instance.\n\n\napproximate\nbool\nTrue\nTrue to use approximate nearest neighbor and False to use brute force. Default to True.\n\n\nReturns\nNone\n\n\n\n\n\nUsage: The ANN filter is usually used when specifying query models.\nBy default, the ANN operator uses approximate nearest neighbor:\n\nmatch_filter = ANN(\n    doc_vector=\"doc_vector\",\n    query_vector=\"query_vector\",\n    hits=10,\n    label=\"label\",\n)\n\nBrute-force can be used by specifying approximate=False:\n\nann_filter = ANN(\n    doc_vector=\"doc_vector\",\n    query_vector=\"query_vector\",\n    hits=10,\n    label=\"label\",\n    approximate=False,\n)\n\n\nsource\n\n\n\n\n Union (*args:__main__.MatchFilter)\n\nMatch documents that belongs to the union of many match filters.\n\n\n\n\nType\nDetails\n\n\n\n\nargs\nMatchFilter\n\n\n\nReturns\nNone\nMatch filters to be taken the union of.\n\n\n\nUsage: The Union filter is usually used when specifying query models.\n\nunion_filter = Union(\n    WeakAnd(hits=10, field=\"field_name\"),\n    ANN(\n        doc_vector=\"doc_vector\",\n        query_vector=\"query_vector\",\n        hits=10,\n        label=\"label\",\n    ),\n)"
  },
  {
    "objectID": "module_query.html#ranking",
    "href": "module_query.html#ranking",
    "title": "query",
    "section": "Ranking",
    "text": "Ranking\n\nsource\n\nRanking\n\n Ranking (name:str='default', list_features:bool=False)\n\nDefine the rank profile to be used during ranking.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nname\nstr\ndefault\nName of the rank profile as defined in a Vespa search definition.\n\n\nlist_features\nbool\nFalse\nShould the ranking features be returned. Either ‘true’ or ‘false’.\n\n\nReturns\nNone\n\n\n\n\n\nUsage: Ranking is usually used when specifying query models.\n\nranking = Ranking(name=\"bm25\", list_features=True)"
  },
  {
    "objectID": "module_query.html#query-properties",
    "href": "module_query.html#query-properties",
    "title": "query",
    "section": "Query properties",
    "text": "Query properties\n\nsource\n\nQueryProperty\n\n QueryProperty ()\n\nAbstract class for query property.\n\nsource\n\n\nQueryRankingFeature\n\n QueryRankingFeature (name:str, mapping:Callable[[str],List[float]])\n\nInclude ranking.feature.query into a Vespa query.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nname\nstr\nName of the feature.\n\n\nmapping\ntyping.Callable[[str], typing.List[float]]\nFunction mapping a string to a list of floats.\n\n\nReturns\nNone\n\n\n\n\nUsage: QueryRankingFeature is usually used when specifying query models.\n\nquery_property = QueryRankingFeature(\n    name=\"query_vector\", mapping=lambda x: [1, 2, 3]\n)"
  },
  {
    "objectID": "module_query.html#query-model",
    "href": "module_query.html#query-model",
    "title": "query",
    "section": "Query model",
    "text": "Query model\n\nsource\n\nQueryModel\n\n QueryModel (name:str='default_name',\n             query_properties:Optional[List[__main__.QueryProperty]]=None,\n             match_phase:__main__.MatchFilter=<__main__.AND object at\n             0x7f9087eae6a0>, ranking:__main__.Ranking=<__main__.Ranking\n             object at 0x7f9087eae640>,\n             body_function:Optional[Callable[[str],Dict]]=None)\n\nDefine a query model.\nA QueryModel is an abstraction that encapsulates all the relevant information controlling how a Vespa app matches and ranks documents.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nname\nstr\ndefault_name\nName of the query model. Used to tag model-related quantities, like evaluation metrics.\n\n\nquery_properties\ntyping.Optional[typing.List[main.QueryProperty]]\nNone\nQuery properties to be included in the queries.\n\n\nmatch_phase\nMatchFilter\n<main.AND object at 0x7f9087eae6a0>\nDefine the match criteria.\n\n\nranking\nRanking\n<main.Ranking object at 0x7f9087eae640>\nDefine the rank criteria.\n\n\nbody_function\ntyping.Optional[typing.Callable[[str], typing.Dict]]\nNone\nFunction that take query as parameter and returns the body of a Vespa query.\n\n\nReturns\nNone\n\n\n\n\n\nUsage:\nSpecify a query model with default configurations:\n\nquery_model = QueryModel()\n\nSpecify match phase, ranking phase and properties used by them.\n\nquery_model = QueryModel(\n    query_properties=[\n        QueryRankingFeature(name=\"query_embedding\", mapping=lambda x: [1, 2, 3])\n    ],\n    match_phase=ANN(\n        doc_vector=\"document_embedding\",\n        query_vector=\"query_embedding\",\n        hits=10,\n        label=\"label\",\n    ),\n    ranking=Ranking(name=\"bm25_plus_embeddings\", list_features=True),\n)\n\nSpecify a query model based on a function that output Vespa YQL.\n\ndef body_function(query):\n    body = {\n        \"yql\": \"select * from sources * where userQuery();\",\n        \"query\": query,\n        \"type\": \"any\",\n        \"ranking\": {\"profile\": \"bm25\", \"listFeatures\": \"true\"},\n    }\n    return body\n\nquery_model = QueryModel(body_function=body_function)"
  },
  {
    "objectID": "module_query.html#send-query-with-querymodel",
    "href": "module_query.html#send-query-with-querymodel",
    "title": "query",
    "section": "Send query with QueryModel",
    "text": "Send query with QueryModel\n\nsource\n\nsend_query\n\n send_query (app:vespa.application.Vespa, body:Optional[Dict]=None,\n             query:Optional[str]=None,\n             query_model:Optional[__main__.QueryModel]=None,\n             debug_request:bool=False, recall:Optional[Tuple]=None,\n             **kwargs)\n\nSend a query request to a Vespa application.\nEither send ‘body’ containing all the request parameters or specify ‘query’ and ‘query_model’.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\napp\nVespa\n\nConnection to a Vespa application\n\n\nbody\ntyping.Optional[typing.Dict]\nNone\nContains all the request parameters. None when using query_model.\n\n\nquery\ntyping.Optional[str]\nNone\nQuery string. None when using body.\n\n\nquery_model\ntyping.Optional[main.QueryModel]\nNone\nQuery model. None when using body.\n\n\ndebug_request\nbool\nFalse\nReturn request body for debugging instead of sending the request.\n\n\nrecall\ntyping.Optional[typing.Tuple]\nNone\nTuple of size 2 where the first element is the name of the field to use to recall and the second element is a list of the values to be recalled.\n\n\nkwargs\n\n\n\n\n\nReturns\nVespaQueryResponse\n\nEither the request body if debug_request is True or the result from the Vespa application.\n\n\n\nUsage: Assume app is a Vespa connection.\nSend request body.\n\nbody = {\"yql\": \"select * from sources * where test\"}\nresult = send_query(app=app, body=body)\n\nUse query and query_model:\n\nresult = send_query(\n    app=app,\n    query=\"this is a test\",\n    query_model=QueryModel(\n        match_phase=OR(), \n        ranking=Ranking()\n    ),\n    hits=10,\n)\n\nDebug the output of the QueryModel by setting debug_request=True:\n\nsend_query(\n    app=app,\n    query=\"this is a test\",\n    query_model=QueryModel(match_phase=OR(), ranking=Ranking()),\n    debug_request=True,\n    hits=10,\n).request_body\n\n{'yql': 'select * from sources * where ({grammar: \"any\"}userInput(\"this is a test\"));',\n 'ranking': {'profile': 'default', 'listFeatures': 'false'},\n 'hits': 10}\n\n\nRecall documents using the id field:\n\nresult = send_query(\n    app=app,\n    query=\"this is a test\",\n    query_model=QueryModel(match_phase=OR(), ranking=Ranking()),\n    hits=10,\n    recall=(\"id\", [1, 5]),\n)\n\nUse a body_function to specify a QueryModel:\n\ndef body_function(query):\n    body = {\n        \"yql\": \"select * from sources * where userQuery();\",\n        \"query\": query,\n        \"type\": \"any\",\n        \"ranking\": {\"profile\": \"bm25\", \"listFeatures\": \"true\"},\n    }\n    return body\n\nquery_model = QueryModel(body_function=body_function)\n\nresult = send_query(\n        app=app,\n        query=\"this is a test\",\n        query_model=query_model,\n        hits=10\n)\n\n\nsource\n\n\nsend_query_batch\n\n send_query_batch (app, body_batch:Optional[List[Dict]]=None,\n                   query_batch:Optional[List[str]]=None,\n                   query_model:Optional[__main__.QueryModel]=None,\n                   recall_batch:Optional[List[Tuple]]=None,\n                   asynchronous=True, connections:Optional[int]=100,\n                   total_timeout:int=100, **kwargs)\n\nSend queries in batch to a Vespa app.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\napp\n\n\nConnection to a Vespa application\n\n\nbody_batch\ntyping.Optional[typing.List[typing.Dict]]\nNone\nContains all the request parameters. Set to None if using ‘query_batch’.\n\n\nquery_batch\ntyping.Optional[typing.List[str]]\nNone\nQuery strings. Set to None if using ‘body_batch’.\n\n\nquery_model\ntyping.Optional[main.QueryModel]\nNone\nQuery model to use when sending query strings. Set to None if using ‘body_batch’.\n\n\nrecall_batch\ntyping.Optional[typing.List[typing.Tuple]]\nNone\nOne tuple for each query. Tuple of size 2 where the first element is the name of the field to use to recall and the second element is a list of the values to be recalled.\n\n\nasynchronous\nbool\nTrue\nSet True to send data in async mode. Default to True.\n\n\nconnections\ntyping.Optional[int]\n100\nNumber of allowed concurrent connections, valid only if asynchronous=True.\n\n\ntotal_timeout\nint\n100\nTotal timeout in secs for each of the concurrent requests when using asynchronous=True.\n\n\nkwargs\n\n\n\n\n\nReturns\ntyping.List[vespa.io.VespaQueryResponse]\n\nHTTP POST responses.\n\n\n\nUse body_batch to send a batch of body requests.\n\nbody_batch = [\n    {\"yql\": \"select * from sources * where test\"},\n    {\"yql\": \"select * from sources * where test2\"}\n]\nresult = send_query_batch(app=app, body_batch=body_batch)\n\nUse query_batch to send a batch of query strings to be ranked according a QueryModel.\n\nresult = send_query_batch(\n    app=app,\n    query_batch=[\"this is a test\", \"this is a test 2\"],\n    query_model=QueryModel(\n        match_phase=OR(), \n        ranking=Ranking()\n    ),\n    hits=10,\n)\n\nUse recall_batch to send one tuple for each query in query_batch.\n\nresult = send_query_batch(\n    app=app,\n    query_batch=[\"this is a test\", \"this is a test 2\"],\n    query_model=QueryModel(match_phase=OR(), ranking=Ranking()),\n    hits=10,\n    recall_batch=[(\"doc_id\", [2, 7]), (\"doc_id\", [0, 5])],\n)"
  },
  {
    "objectID": "module_query.html#collect-vespa-features",
    "href": "module_query.html#collect-vespa-features",
    "title": "query",
    "section": "Collect Vespa features",
    "text": "Collect Vespa features\n\nsource\n\ncollect_vespa_features\n\n collect_vespa_features (app:vespa.application.Vespa, labeled_data,\n                         id_field:str, query_model:__main__.QueryModel,\n                         number_additional_docs:int, fields:List[str],\n                         keep_features:Optional[List[str]]=None,\n                         relevant_score:int=1, default_score:int=0,\n                         **kwargs)\n\nCollect Vespa features based on a set of labelled data.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\napp\nVespa\n\nConnection to a Vespa application.\n\n\nlabeled_data\n\n\nLabelled data containing query, query_id and relevant ids. See examples about data format.\n\n\nid_field\nstr\n\nThe Vespa field representing the document id.\n\n\nquery_model\nQueryModel\n\nQuery model.\n\n\nnumber_additional_docs\nint\n\nNumber of additional documents to retrieve for each relevant document. Duplicate documents will be dropped.\n\n\nfields\ntyping.List[str]\n\nVespa fields to collect, e.g. [“rankfeatures”, “summaryfeatures”]\n\n\nkeep_features\ntyping.Optional[typing.List[str]]\nNone\nList containing the names of the features that should be returned. Default to None, which return all the features contained in the ‘fields’ argument.\n\n\nrelevant_score\nint\n1\nScore to assign to relevant documents. Default to 1.\n\n\ndefault_score\nint\n0\nScore to assign to the additional documents that are not relevant. Default to 0.\n\n\nkwargs\n\n\n\n\n\nReturns\nDataFrame\n\nDataFrame containing document id (document_id), query id (query_id), scores (relevant) and vespa rank features returned by the Query model RankProfile used.\n\n\n\nUsage:\nDefine labeled_data as a list of dict containing relevant documents:\n\nlabeled_data = [\n    {\n        \"query_id\": 0,\n        \"query\": \"give me title 1\",\n        \"relevant_docs\": [{\"id\": \"1\", \"score\": 1}],\n    },\n    {\n        \"query_id\": 1,\n        \"query\": \"give me title 3\",\n        \"relevant_docs\": [{\"id\": \"3\", \"score\": 1}],\n    },\n]\n\nCollect vespa features:\n\nrank_features = collect_vespa_features(\n    app=app,\n    labeled_data=labeled_data,\n    id_field=\"doc_id\",\n    query_model=QueryModel(\n        match_phase=OR(), \n        ranking=Ranking(name=\"bm25\", list_features=True)\n    ),\n    number_additional_docs=2,\n    fields=[\"rankfeatures\"],\n)\nrank_features\n\n\n\n\n\n  \n    \n      \n      document_id\n      query_id\n      label\n      attributeMatch(doc_id)\n      attributeMatch(doc_id).averageWeight\n      attributeMatch(doc_id).completeness\n      attributeMatch(doc_id).fieldCompleteness\n      attributeMatch(doc_id).importance\n      attributeMatch(doc_id).matches\n      attributeMatch(doc_id).maxWeight\n      ...\n      term(3).significance\n      term(3).weight\n      term(4).connectedness\n      term(4).significance\n      term(4).weight\n      textSimilarity(text).fieldCoverage\n      textSimilarity(text).order\n      textSimilarity(text).proximity\n      textSimilarity(text).queryCoverage\n      textSimilarity(text).score\n    \n  \n  \n    \n      0\n      1\n      0\n      1\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.583333\n      100.0\n      0.0\n      0.0\n      0.0\n      0.50\n      1.0\n      1.000000\n      0.50\n      0.750000\n    \n    \n      3\n      7\n      0\n      0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.583333\n      100.0\n      0.0\n      0.0\n      0.0\n      0.25\n      0.0\n      0.859375\n      0.25\n      0.425781\n    \n    \n      1\n      3\n      1\n      1\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.583333\n      100.0\n      0.0\n      0.0\n      0.0\n      0.50\n      1.0\n      1.000000\n      0.50\n      0.750000\n    \n    \n      5\n      7\n      1\n      0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.583333\n      100.0\n      0.0\n      0.0\n      0.0\n      0.25\n      0.0\n      0.859375\n      0.25\n      0.425781\n    \n  \n\n4 rows × 94 columns\n\n\n\nUse a DataFrame for labeled_data instead of a list of dict:\n\nlabeled_data = [\n    {\n        \"qid\": 0,\n        \"query\": \"give me title 1\",\n        \"doc_id\": 1, \n        \"relevance\": 1\n    },\n    {\n        \"qid\": 1,\n        \"query\": \"give me title 3\",\n        \"doc_id\": 3, \n        \"relevance\": 1\n    },\n]\nlabeled_data_df = DataFrame.from_records(labeled_data)\nlabeled_data_df\n\n\n\n\n\n  \n    \n      \n      qid\n      query\n      doc_id\n      relevance\n    \n  \n  \n    \n      0\n      0\n      give me title 1\n      1\n      1\n    \n    \n      1\n      1\n      give me title 3\n      3\n      1\n    \n  \n\n\n\n\n\nrank_features = collect_vespa_features(\n    app=app,\n    labeled_data=labeled_data_df,\n    id_field=\"doc_id\",\n    query_model=QueryModel(\n        match_phase=OR(), ranking=Ranking(name=\"bm25\", list_features=True)\n    ),\n    number_additional_docs=2,\n    fields=[\"rankfeatures\"],\n)\nrank_features\n\n\n\n\n\n  \n    \n      \n      document_id\n      query_id\n      label\n      attributeMatch(doc_id)\n      attributeMatch(doc_id).averageWeight\n      attributeMatch(doc_id).completeness\n      attributeMatch(doc_id).fieldCompleteness\n      attributeMatch(doc_id).importance\n      attributeMatch(doc_id).matches\n      attributeMatch(doc_id).maxWeight\n      ...\n      term(3).significance\n      term(3).weight\n      term(4).connectedness\n      term(4).significance\n      term(4).weight\n      textSimilarity(text).fieldCoverage\n      textSimilarity(text).order\n      textSimilarity(text).proximity\n      textSimilarity(text).queryCoverage\n      textSimilarity(text).score\n    \n  \n  \n    \n      0\n      1\n      0\n      0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.583333\n      100.0\n      0.0\n      0.0\n      0.0\n      0.50\n      1.0\n      1.000000\n      0.50\n      0.750000\n    \n    \n      3\n      7\n      0\n      0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.583333\n      100.0\n      0.0\n      0.0\n      0.0\n      0.25\n      0.0\n      0.859375\n      0.25\n      0.425781\n    \n    \n      1\n      3\n      1\n      0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.583333\n      100.0\n      0.0\n      0.0\n      0.0\n      0.50\n      1.0\n      1.000000\n      0.50\n      0.750000\n    \n    \n      5\n      7\n      1\n      0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.583333\n      100.0\n      0.0\n      0.0\n      0.0\n      0.25\n      0.0\n      0.859375\n      0.25\n      0.425781\n    \n  \n\n4 rows × 94 columns\n\n\n\nKeep only selected features by specifying their names in the keep_features argument:\n\nrank_features = collect_vespa_features(\n    app=app,\n    labeled_data=labeled_data_df,\n    id_field=\"doc_id\",\n    query_model=QueryModel(\n        match_phase=OR(), ranking=Ranking(name=\"bm25\", list_features=True)\n    ),\n    number_additional_docs=2,\n    fields=[\"rankfeatures\"],\n    keep_features=[\"textSimilarity(text).score\"],\n)\nrank_features\n\n\n\n\n\n  \n    \n      \n      document_id\n      query_id\n      label\n      textSimilarity(text).score\n    \n  \n  \n    \n      0\n      1\n      0\n      0\n      0.750000\n    \n    \n      3\n      7\n      0\n      0\n      0.425781\n    \n    \n      1\n      3\n      1\n      0\n      0.750000\n    \n    \n      5\n      7\n      1\n      0\n      0.425781\n    \n  \n\n\n\n\n\nsource\n\n\nstore_vespa_features\n\n store_vespa_features (app:vespa.application.Vespa, output_file_path:str,\n                       labeled_data, id_field:str,\n                       query_model:__main__.QueryModel,\n                       number_additional_docs:int, fields:List[str],\n                       keep_features:Optional[List[str]]=None,\n                       relevant_score:int=1, default_score:int=0,\n                       batch_size=1000, **kwargs)\n\nRetrieve Vespa rank features and store them in a .csv file.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\napp\nVespa\n\nConnection to a Vespa application.\n\n\noutput_file_path\nstr\n\nPath of the .csv output file. It will create the file of it does not exist and append the vespa features to an pre-existing file.\n\n\nlabeled_data\n\n\nLabelled data containing query, query_id and relevant ids. See details about data format.\n\n\nid_field\nstr\n\nThe Vespa field representing the document id.\n\n\nquery_model\nQueryModel\n\nQuery model.\n\n\nnumber_additional_docs\nint\n\nNumber of additional documents to retrieve for each relevant document.\n\n\nfields\ntyping.List[str]\n\nList of Vespa fields to collect, e.g. [“rankfeatures”, “summaryfeatures”]\n\n\nkeep_features\ntyping.Optional[typing.List[str]]\nNone\nList containing the names of the features that should be returned. Default to None, which return all the features contained in the ‘fields’ argument.\n\n\nrelevant_score\nint\n1\nScore to assign to relevant documents.\n\n\ndefault_score\nint\n0\nScore to assign to the additional documents that are not relevant.\n\n\nbatch_size\nint\n1000\nThe size of the batch of labeled data points to be processed.\n\n\nkwargs\n\n\n\n\n\nReturns\nint\n\nreturns 0 upon success.\n\n\n\nUsage:\n\nlabeled_data = [\n    {\n        \"query_id\": 0,\n        \"query\": \"give me title 1\",\n        \"relevant_docs\": [{\"id\": \"1\", \"score\": 1}],\n    },\n    {\n        \"query_id\": 1,\n        \"query\": \"give me title 3\",\n        \"relevant_docs\": [{\"id\": \"3\", \"score\": 1}],\n    },\n]\n\nstore_vespa_features(\n    app=app,\n    output_file_path=\"vespa_features.csv\",\n    labeled_data=labeled_data,\n    id_field=\"doc_id\",\n    query_model=QueryModel(\n        match_phase=OR(), ranking=Ranking(name=\"bm25\", list_features=True)\n    ),\n    number_additional_docs=2,\n    fields=[\"rankfeatures\", \"summaryfeatures\"],\n)\nrank_features = read_csv(\"vespa_features.csv\")\nrank_features\n\nRows collected: 4.\nBatch progress: 1/1.\n\n\n\n\n\n\n  \n    \n      \n      document_id\n      query_id\n      label\n      attributeMatch(doc_id)\n      attributeMatch(doc_id).averageWeight\n      attributeMatch(doc_id).completeness\n      attributeMatch(doc_id).fieldCompleteness\n      attributeMatch(doc_id).importance\n      attributeMatch(doc_id).matches\n      attributeMatch(doc_id).maxWeight\n      ...\n      term(3).weight\n      term(4).connectedness\n      term(4).significance\n      term(4).weight\n      textSimilarity(text).fieldCoverage\n      textSimilarity(text).order\n      textSimilarity(text).proximity\n      textSimilarity(text).queryCoverage\n      textSimilarity(text).score\n      vespa.summaryFeatures.cached\n    \n  \n  \n    \n      0\n      1\n      0\n      1\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      100.0\n      0.0\n      0.0\n      0.0\n      0.50\n      1.0\n      1.000000\n      0.50\n      0.750000\n      0.0\n    \n    \n      1\n      7\n      0\n      0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      100.0\n      0.0\n      0.0\n      0.0\n      0.25\n      0.0\n      0.859375\n      0.25\n      0.425781\n      0.0\n    \n    \n      2\n      3\n      1\n      1\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      100.0\n      0.0\n      0.0\n      0.0\n      0.50\n      1.0\n      1.000000\n      0.50\n      0.750000\n      0.0\n    \n    \n      3\n      7\n      1\n      0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      100.0\n      0.0\n      0.0\n      0.0\n      0.25\n      0.0\n      0.859375\n      0.25\n      0.425781\n      0.0\n    \n  \n\n4 rows × 95 columns"
  },
  {
    "objectID": "module_passage.html",
    "href": "module_passage.html",
    "title": "passage",
    "section": "",
    "text": "Code related to the manipulation of passage ranking data.\n\nsource\n\n\n\n sample_dict_items (d:Dict, n:int)\n\nSample items from a dict.\n\n\n\n\nType\nDetails\n\n\n\n\nd\ntyping.Dict\ndict to be samples from.\n\n\nn\nint\nNumber of samples\n\n\nReturns\ntyping.Dict\ndict with sampled values\n\n\n\nUsage:\n\nd = {\"a\": 1, \"b\":2, \"c\":3}\n\n\nsample_dict_items(d, 1)\n\n{'a': 1}\n\n\n\nsample_dict_items(d, 2)\n\n{'c': 3, 'b': 2}\n\n\n\nsample_dict_items(d, 3)\n\n{'a': 1, 'c': 3, 'b': 2}\n\n\nReturn full dict in case number of samples is higher than length of the dict:\n\nsample_dict_items(d, 4)\n\n{'a': 1, 'c': 3, 'b': 2}\n\n\n\nsource\n\n\n\n\n save_data (corpus:Dict, train_qrels:Dict, train_queries:Dict,\n            dev_qrels:Dict, dev_queries:Dict,\n            file_path:str='passage_sample.json')\n\nSave data to disk.\nThe main goal is to save sample data to disk.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncorpus\ntyping.Dict\n\nDocument corpus, see usage example below.\n\n\ntrain_qrels\ntyping.Dict\n\nTraining relevance scores, see usage example below.\n\n\ntrain_queries\ntyping.Dict\n\nTraining queries, see usage example below.\n\n\ndev_qrels\ntyping.Dict\n\nDevelopment relevance scores, see usage example below.\n\n\ndev_queries\ntyping.Dict\n\nDevelopment queries, see usage example below.\n\n\nfile_path\nstr\npassage_sample.json\nValid JSON file path.\n\n\nReturns\nNone\n\nSide-effect: data is saved to file_path.\n\n\n\nUsage:\n\ncorpus = {\n    \"0\": \"sentence 0\", \n    \"1\": \"sentence 1\", \n    \"2\": \"sentence 2\", \n    \"3\": \"sentence 3\"\n}\ntrain_queries = {\n    \"10\": \"train query 10\",\n    \"11\": \"train query 11\"\n}\ntrain_qrels = {\n    \"10\": {\"0\": 1},\n    \"11\": {\"2\": 1}\n}\ndev_queries = {\n    \"20\": \"train query 20\",\n    \"21\": \"train query 21\"\n}\ndev_qrels = {\n    \"20\": {\"1\": 1},\n    \"21\": {\"3\": 1}\n}\n\n\nsave_data(\n    corpus, \n    train_qrels, train_queries, \n    dev_qrels, dev_queries, \n    file_path=\"passage_sample.json\"\n)\n\n\nsource\n\n\n\n\n load_data (file_path:Optional[str]=None)\n\nLoad data.\nThe main goal is to load sample data from disk. If a file_path is not provided, a pre-generated data sample will be downloaded.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\ntyping.Optional[str]\nNone\nvalid JSON file path contain data saved by save_data. If None, a pre-generated sample will be downloaded.\n\n\nReturns\ntyping.Dict\n\nSee usage example below for expected format.\n\n\n\nUsage:\n\nWith file_path:\n\n\ndata = load_data(\"passage_sample.json\")\n\n\ndata\n\n{'corpus': {'0': 'sentence 0',\n  '1': 'sentence 1',\n  '2': 'sentence 2',\n  '3': 'sentence 3'},\n 'train_qrels': {'10': {'0': 1}, '11': {'2': 1}},\n 'train_queries': {'10': 'train query 10', '11': 'train query 11'},\n 'dev_qrels': {'20': {'1': 1}, '21': {'3': 1}},\n 'dev_queries': {'20': 'train query 20', '21': 'train query 21'}}\n\n\n\nWithout file_path specified, a pre-generated sample data will be downloaded:\n\n\ndata = load_data()\n\n\ndata.keys()\n\ndict_keys(['corpus', 'train_qrels', 'train_queries', 'dev_qrels', 'dev_queries'])\n\n\n\nlen(data[\"corpus\"])\n\n1000\n\n\n\nsource\n\n\n\n\n PassageData (corpus:Optional[Dict]=None, train_qrels:Optional[Dict]=None,\n              train_queries:Optional[Dict]=None,\n              dev_qrels:Optional[Dict]=None,\n              dev_queries:Optional[Dict]=None)\n\nContainer for passage data\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncorpus\ntyping.Optional[typing.Dict]\nNone\nDocument corpus, see usage example below.\n\n\ntrain_qrels\ntyping.Optional[typing.Dict]\nNone\nTraining relevance scores, see usage example below.\n\n\ntrain_queries\ntyping.Optional[typing.Dict]\nNone\nTraining queries, see usage example below.\n\n\ndev_qrels\ntyping.Optional[typing.Dict]\nNone\nDevelopment relevance scores, see usage example below.\n\n\ndev_queries\ntyping.Optional[typing.Dict]\nNone\nDevelopment queries, see usage example below.\n\n\n\nUsage:\n\ncorpus = {\n    \"0\": \"sentence 0\", \n    \"1\": \"sentence 1\", \n    \"2\": \"sentence 2\", \n    \"3\": \"sentence 3\"\n}\ntrain_queries = {\n    \"10\": \"train query 10\",\n    \"11\": \"train query 11\"\n}\ntrain_qrels = {\n    \"10\": {\"0\": 1},\n    \"11\": {\"2\": 1}\n}\ndev_queries = {\n    \"20\": \"train query 20\",\n    \"21\": \"train query 21\"\n}\ndev_qrels = {\n    \"20\": {\"1\": 1},\n    \"21\": {\"3\": 1}\n}\n\n\npassage_data = PassageData(\n    corpus=corpus, \n    train_queries = train_queries, \n    train_qrels=train_qrels,\n    dev_queries = dev_queries,\n    dev_qrels = dev_qrels\n)\n\n\npassage_data\n\nPassageData(corpus, train_qrels, train_queries, dev_qrels, dev_queries)\n\n\n\nsource\n\n\n\n\n PassageData.save (file_path:str='passage_sample.json')\n\n\npassage_data.save()\n\n\nsource\n\n\n\n\n PassageData.load (file_path:Optional[str]=None)\n\nLoad passage data from disk.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\ntyping.Optional[str]\nNone\nvalid JSON file path contain data saved by save_data. If None, a pre-generated sample will be downloaded.\n\n\nReturns\nPassageData\n\n\n\n\n\n\ndata = PassageData.load(file_path=\"passage_sample.json\")\n\n\ndata\n\nPassageData(corpus, train_qrels, train_queries, dev_qrels, dev_queries)\n\n\n\nsource\n\n\n\n\n PassageData.summary ()\n\nSummary of the size of the dataset components.\n\ndata.summary\n\nNumber of documents: 4\nNumber of train queries: 2\nNumber of train relevance judgments: 2\nNumber of dev queries: 2\nNumber of dev relevance judgments: 2\n\n\n\nsource\n\n\n\n\n PassageData.get_corpus ()\n\n\npassage_data.get_corpus()\n\n\n\n\n\n  \n    \n      \n      doc_id\n      text\n    \n  \n  \n    \n      0\n      0\n      sentence 0\n    \n    \n      1\n      1\n      sentence 1\n    \n    \n      2\n      2\n      sentence 2\n    \n    \n      3\n      3\n      sentence 3\n    \n  \n\n\n\n\n\nsource\n\n\n\n\n PassageData.get_queries (type:str)\n\nGet query data.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntype\nstr\nEither ‘train’ or ‘dev’.\n\n\nReturns\nDataFrame\nDataFrame conaining ‘query_id’ and ‘query’.\n\n\n\n\npassage_data.get_queries(type=\"train\")\n\n\n\n\n\n  \n    \n      \n      query_id\n      query\n    \n  \n  \n    \n      0\n      10\n      train query 10\n    \n    \n      1\n      11\n      train query 11\n    \n  \n\n\n\n\n\npassage_data.get_queries(type=\"dev\")\n\n\n\n\n\n  \n    \n      \n      query_id\n      query\n    \n  \n  \n    \n      0\n      20\n      train query 20\n    \n    \n      1\n      21\n      train query 21\n    \n  \n\n\n\n\n\nsource\n\n\n\n\n PassageData.get_labels (type:str)\n\nGet labeled data\n\n\n\n\nType\nDetails\n\n\n\n\ntype\nstr\nEither ‘train’ or ‘dev’.\n\n\nReturns\ntyping.Dict\npyvespa-formatted labeled data\n\n\n\n\npassage_data.get_labels(type=\"train\")\n\n[{'query_id': '10',\n  'query': 'train query 10',\n  'relevant_docs': [{'id': '0', 'score': 1}]},\n {'query_id': '11',\n  'query': 'train query 11',\n  'relevant_docs': [{'id': '2', 'score': 1}]}]\n\n\n\npassage_data.get_labels(type=\"dev\")\n\n[{'query_id': '20',\n  'query': 'train query 20',\n  'relevant_docs': [{'id': '1', 'score': 1}]},\n {'query_id': '21',\n  'query': 'train query 21',\n  'relevant_docs': [{'id': '3', 'score': 1}]}]\n\n\n\nsource\n\n\n\n\n sample_data (n_relevant:int, n_irrelevant:int)\n\nSample data from the passage ranking dataset.\nThe final sample contains n_relevant train relevant documents, n_relevant dev relevant documents and n_irrelevant random documents sampled from the entire corpus.\nAll the relevant sampled documents, both from train and dev sets, are guaranteed to be on the corpus_sample, which will contain 2 * n_relevant + n_irrelevant documents.\n\n\n\n\nType\nDetails\n\n\n\n\nn_relevant\nint\nThe number of relevant documents to sample.\n\n\nn_irrelevant\nint\nThe number of non-judged documents to sample.\n\n\nReturns\nPassageData\n\n\n\n\nUsage:\n\nsample = sample_data(n_relevant=1, n_irrelevant=3)\n\nThe sampled corpus is a dict containing document id as key and the passage text as value.\n\nsample.corpus\n\n{'890370': 'the map of europe gives you a clear view of the political boundaries that segregate the countries in the continent including germany uk france spain italy greece romania ukraine hungary austria sweden finland norway czech republic belgium luxembourg switzerland croatia and albaniahe map of europe gives you a clear view of the political boundaries that segregate the countries in the continent including germany uk france spain italy greece romania ukraine hungary austria sweden finland norway czech republic belgium luxembourg switzerland croatia and albania',\n '5060205': 'Setting custom HTTP headers with cURL can be done by using the CURLOPT_HTTPHEADER option, which can be set with the curl_setopt function. To add headers to your HTTP request you need to put them into a PHP Array, which you can then pass to the cul_setopt function, like demonstrated in the below example.',\n '6096573': \"The sugar in RNA is ribose, whereas the sugar in DNA is deoxyribose. The only difference between the two is that in deoxyribose, there is an oxygen missing from the 2' carbon …(there is a H there instead of an OH). This makes DNA more stable/less reactive than RNA. 1 person found this useful.\",\n '3092885': 'All three C-Ph bonds are typical of sp 3 - sp 2 carbon-carbon bonds with lengths of approximately 1.47 A, å while The-C o bond length is approximately.1 42. A å the presence of three adjacent phenyl groups confers special properties manifested in the reactivity of. the alcohol',\n '7275560': 'shortest phase of mitosis Anaphase is the shortest phase of mitosis. During anaphase the arranged chromosomes at the metaphase plate are migrate towards their respective poles. Before this migration started, chromosomes are divided into sister chromatids, by the separation of joined centromere of two sister chromatids of a chromosomes.'}\n\n\nThe size of the sampled document corpus is equal to 2 * n_relevant + n_irrelevant.\n\nlen(sample.corpus)\n\n5\n\n\nSampled queries are dict containing query id as key and query text as value.\n\nprint(sample.train_queries)\nprint(sample.dev_queries)\n\n{'899723': 'what sugar is found in rna'}\n{'994205': 'which is the shortest stage in duration'}\n\n\nSampled qrels contains one relevant document for each query.\n\nprint(sample.train_qrels)\nprint(sample.dev_qrels)\n\n{'899723': {'6096573': 1}}\n{'994205': {'7275560': 1}}\n\n\nThe following relevant documents are guaranteed to be included in the corpus_sample.\n\n\n['6096573', '7275560']"
  },
  {
    "objectID": "module_passage.html#basic-search",
    "href": "module_passage.html#basic-search",
    "title": "passage",
    "section": "Basic search",
    "text": "Basic search\nCode related to a basic search search engine for passage ranking.\n\nsource\n\ncreate_basic_search_package\n\n create_basic_search_package (name:str='PassageRanking')\n\nCreate a basic Vespa application package for passage ranking.\nVespa fields:\nThe application contain two string fields: doc_id and text.\nVespa rank functions:\nThe application contain two rank profiles: bm25 and nativeRank.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nname\nstr\nPassageRanking\nName of the application\n\n\nReturns\nApplicationPackage\n\npyvespa ApplicationPackage instance.\n\n\n\nUsage:\n\napp_package = create_basic_search_package(name=\"PassageModuleApp\")\n\nCheck how the Vespa schema definition for this application looks like:\n\nprint(app_package.schema.schema_to_text)\n\nschema PassageModuleApp {\n    document PassageModuleApp {\n        field doc_id type string {\n            indexing: attribute | summary\n        }\n        field text type string {\n            indexing: index | summary\n            index: enable-bm25\n        }\n    }\n    fieldset default {\n        fields: text\n    }\n    rank-profile bm25 {\n        first-phase {\n            expression: bm25(text)\n        }\n        summary-features {\n            bm25(text)\n        }\n    }\n    rank-profile native_rank {\n        first-phase {\n            expression: nativeRank(text)\n        }\n    }\n}"
  },
  {
    "objectID": "module_passage.html#evaluate-query-models",
    "href": "module_passage.html#evaluate-query-models",
    "title": "passage",
    "section": "Evaluate query models",
    "text": "Evaluate query models\n\nsource\n\nevaluate_query_models\n\n evaluate_query_models (app_package:vespa.package.ApplicationPackage,\n                        query_models:List[learntorank.query.QueryModel],\n                        metrics:List[learntorank.evaluation.EvalMetric],\n                        corpus_size:List[int], output_file_path:str,\n                        dev_query_percentage:float=0.006285807802305023,\n                        verbose:bool=True, **kwargs)\n\n\nfrom learntorank.evaluation import (\n    MatchRatio,\n    Recall, \n    ReciprocalRank, \n    NormalizedDiscountedCumulativeGain\n)\nfrom learntorank.query import QueryModel, OR, Ranking\n\ncorpus_size = [100, 200]\napp_package = create_basic_search_package(name=\"PassageEvaluationApp\")\nquery_models = [\n    QueryModel(\n        name=\"bm25\", \n        match_phase=OR(), \n        ranking=Ranking(name=\"bm25\")\n    ),\n    QueryModel(\n        name=\"native_rank\", \n        match_phase=OR(), \n        ranking=Ranking(name=\"native_rank\")\n    )\n]\nmetrics = [\n    MatchRatio(),\n    Recall(at=100), \n    ReciprocalRank(at=10), \n    NormalizedDiscountedCumulativeGain(at=10)\n]\noutput_file_path = \"test.csv\"\n\n\nestimates = evaluate_query_models(\n    app_package=app_package,\n    query_models=query_models,\n    metrics=metrics,\n    corpus_size=corpus_size,\n    dev_query_percentage=0.5,\n    output_file_path=output_file_path, \n    verbose=False\n)\n\n*****\nDeploy Vespa application:\n*****\nWaiting for configuration server, 0/300 seconds...\nWaiting for configuration server, 5/300 seconds...\nWaiting for configuration server, 10/300 seconds...\nWaiting for application status, 0/300 seconds...\nWaiting for application status, 5/300 seconds...\nWaiting for application status, 10/300 seconds...\nWaiting for application status, 15/300 seconds...\nWaiting for application status, 20/300 seconds...\nWaiting for application status, 25/300 seconds...\nWaiting for application status, 30/300 seconds...\nWaiting for application status, 35/300 seconds...\nWaiting for application status, 40/300 seconds...\nWaiting for application status, 45/300 seconds...\nWaiting for application status, 50/300 seconds...\nWaiting for application status, 55/300 seconds...\nWaiting for application status, 60/300 seconds...\nWaiting for application status, 65/300 seconds...\nWaiting for application status, 70/300 seconds...\nWaiting for application status, 75/300 seconds...\nWaiting for application status, 80/300 seconds..."
  },
  {
    "objectID": "module_ml.html",
    "href": "module_ml.html",
    "title": "ml",
    "section": "",
    "text": "source\n\n\n\n Task (model_id:str)\n\nBase class for ML Tasks.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nmodel_id\nstr\nId used to identify the model on Vespa applications.\n\n\n\n\nsource\n\n\n\n\n TextTask (model_id:str, model:str, tokenizer:Optional[str]=None,\n           output_file:<class'IO'>=<_io.StringIO object at\n           0x7f9087e90790>)\n\nBase class for Tasks involving text inputs.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel_id\nstr\n\nId used to identify the model on Vespa applications.\n\n\nmodel\nstr\n\nId of the model as used by the model hub.\n\n\ntokenizer\ntyping.Optional[str]\nNone\nId of the tokenizer as used by the model hub.\n\n\noutput_file\nIO\n<_io.StringIO object at 0x7f9087e90790>\nOutput file to write output messages.\n\n\n\n\nsource\n\n\n\n\n TextTask.export_to_onnx (output_path:str)\n\nExport a model to ONNX\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\noutput_path\nstr\nRelative output path for the onnx model, should end in ‘.onnx’\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\n\n\n TextTask.predict (text:str)\n\nPredict using a local instance of the model\n\n\n\n\nType\nDetails\n\n\n\n\ntext\nstr\ntext input for the task.\n\n\nReturns\ntyping.List\nPredictions.\n\n\n\n\nsource\n\n\n\n\n SequenceClassification (model_id:str, model:str,\n                         tokenizer:Optional[str]=None,\n                         output_file:<class'IO'>=<_io.StringIO object at\n                         0x7f8fec52dd30>)\n\nSequence Classification task.\nIt takes a text input and returns an array of floats depending on which model is used to solve the task.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel_id\nstr\n\nId used to identify the model on Vespa applications.\n\n\nmodel\nstr\n\nId of the model as used by the model hub. Alternatively, it can also be the path to the folder containing the model files, as long as the model config is also there.\n\n\ntokenizer\ntyping.Optional[str]\nNone\nId of the tokenizer as used by the model hub. Alternatively, it can also be the path to the folder containing the tokenizer files, as long as the model config is also there.\n\n\noutput_file\nIO\n<_io.StringIO object at 0x7f8fec52dd30>\nOutput file to write output messages."
  },
  {
    "objectID": "module_ml.html#model-config-for-vespa-applications",
    "href": "module_ml.html#model-config-for-vespa-applications",
    "title": "ml",
    "section": "Model config for Vespa applications",
    "text": "Model config for Vespa applications\n\nsource\n\nModelConfig\n\n ModelConfig (model_id)\n\nBase model configuration for Vespa applications.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nmodel_id\n\nUnique model id to represent the model within a Vespa application.\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nBertModelConfig\n\n BertModelConfig (model_id:str, query_input_size:int, doc_input_size:int,\n                  tokenizer:Union[str,os.PathLike],\n                  model:Union[str,os.PathLike,NoneType]=None)\n\nBERT model configuration for Vespa applications.\n\n\n\nbert_config = BertModelConfig( … model_id=“pretrained_bert_tiny”, … query_input_size=32, … doc_input_size=96, … tokenizer=“google/bert_uncased_L-2_H-128_A-2”, … model=“google/bert_uncased_L-2_H-128_A-2”, … ) # doctest: +SKIP BertModelConfig(‘pretrained_bert_tiny’, 32, 96, ‘google/bert_uncased_L-2_H-128_A-2’, ‘google/bert_uncased_L-2_H-128_A-2’)\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel_id\nstr\n\nUnique model id to represent the model within a Vespa application.\n\n\nquery_input_size\nint\n\nThe size of the input vector dedicated to the query text.\n\n\ndoc_input_size\nint\n\nThe size of the input vector dedicated to the document text.\n\n\ntokenizer\ntyping.Union[str, os.PathLike]\n\nThe name or a path to a saved BERT model tokenizer from the transformers library.\n\n\nmodel\ntyping.Union[str, os.PathLike, NoneType]\nNone\nThe name or a path to a saved model that is compatible with the tokenizer. The model is optional at construction since you might want to train it first. You must add a model via :func:add_model before deploying a Vespa application that uses this class.\n\n\nReturns\nNone\n\n\n\n\n\n\n#bert_config = BertModelConfig(\n#    model_id=\"pretrained_bert_tiny\",\n#    query_input_size=32,\n#    doc_input_size=96,\n#    tokenizer=\"google/bert_uncased_L-2_H-128_A-2\"\n#)\n\n\n#bert_config = BertModelConfig(\n#    model_id=\"pretrained_bert_tiny\",\n#    query_input_size=32,\n#    doc_input_size=96,\n#    tokenizer=\"google/bert_uncased_L-2_H-128_A-2\",\n#    model=\"google/bert_uncased_L-2_H-128_A-2\",\n#)\n\n\nsource\n\n\nBertModelConfig.predict\n\n BertModelConfig.predict (queries, docs)\n\nPredict (forward pass) given queries and docs texts\n\n\n\n\nType\nDetails\n\n\n\n\nqueries\n\nA List of query texts.\n\n\ndocs\n\nA List of document texts.\n\n\nReturns\ntyping.List\nLogits\n\n\n\n\nsource\n\n\nBertModelConfig.add_model\n\n BertModelConfig.add_model (model:Union[str,os.PathLike])\n\nAdd a BERT model\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nmodel\ntyping.Union[str, os.PathLike]\nThe name or a path to a saved model that is compatible with the tokenizer.\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nBertModelConfig.doc_fields\n\n BertModelConfig.doc_fields (text:str)\n\nGenerate document fields related to the model that needs to be fed to Vespa.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntext\nstr\nThe text related to the document to be used as input to the bert model\n\n\nReturns\ntyping.Dict\nDict with key and values as expected by Vespa.\n\n\n\n\nsource\n\n\nBertModelConfig.query_tensor_mapping\n\n BertModelConfig.query_tensor_mapping (text:str)\n\nMaps query text to a tensor expected by Vespa at run time.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntext\nstr\nQuery text to be used as input to the BERT model.\n\n\nReturns\ntyping.List[float]\nInput ids expected by Vespa.\n\n\n\n\nsource\n\n\nBertModelConfig.create_encodings\n\n BertModelConfig.create_encodings (queries:List[str], docs:List[str],\n                                   return_tensors=False)\n\nCreate BERT model encodings.\nCreate BERT encodings following the same pattern used during Vespa serving. Useful to generate training data and ensuring training and serving compatibility.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nqueries\ntyping.List[str]\n\nQuery texts.\n\n\ndocs\ntyping.List[str]\n\nDocument texts.\n\n\nreturn_tensors\nbool\nFalse\nReturn tensors\n\n\nReturns\ntyping.Dict\n\nDict containing input_ids, token_type_ids and attention_mask encodings.\n\n\n\n\nsource\n\n\nBertModelConfig.export_to_onnx\n\n BertModelConfig.export_to_onnx (output_path:str)\n\nExport a model to ONNX\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\noutput_path\nstr\nRelative output path for the onnx model, should end in ‘.onnx’\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nBertModelConfig.onnx_model\n\n BertModelConfig.onnx_model ()\n\n\nsource\n\n\nBertModelConfig.query_profile_type_fields\n\n BertModelConfig.query_profile_type_fields ()\n\n\nsource\n\n\nBertModelConfig.document_fields\n\n BertModelConfig.document_fields (document_field_indexing)\n\n\nsource\n\n\nBertModelConfig.rank_profile\n\n BertModelConfig.rank_profile (include_model_summary_features, **kwargs)"
  },
  {
    "objectID": "module_ml.html#model-server",
    "href": "module_ml.html#model-server",
    "title": "ml",
    "section": "Model Server",
    "text": "Model Server\n\nsource\n\nModelServer\n\n ModelServer (name:str, tasks:Optional[List[__main__.Task]]=None)\n\nCreate a Vespa stateless model evaluation server.\nA Vespa stateless model evaluation server is a simplified Vespa application without content clusters.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nname\nstr\n\nApplication name.\n\n\ntasks\ntyping.Optional[typing.List[main.Task]]\nNone\nList of tasks to be served."
  },
  {
    "objectID": "module_ml.html#add-ranking-model",
    "href": "module_ml.html#add-ranking-model",
    "title": "ml",
    "section": "Add ranking model",
    "text": "Add ranking model\n\nsource\n\nadd_ranking_model\n\n add_ranking_model (app_package:vespa.package.ApplicationPackage,\n                    model_config:__main__.ModelConfig, schema=None,\n                    include_model_summary_features=False,\n                    document_field_indexing=None, **kwargs)\n\nAdd ranking profile based on a specific model config.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\napp_package\nApplicationPackage\n\nApplication package to include ranking model\n\n\nmodel_config\nModelConfig\n\nModel config instance specifying the model to be used on the RankProfile.\n\n\nschema\nNoneType\nNone\nName of the schema to add model ranking to.\n\n\ninclude_model_summary_features\nbool\nFalse\nTrue to include model specific summary features, such as inputs and outputs that are useful for debugging. Default to False as this requires an extra model evaluation when fetching summary features.\n\n\ndocument_field_indexing\nNoneType\nNone\nList of indexing attributes for the document fields required by the ranking model.\n\n\nkwargs\n\n\n\n\n\nReturns\nNone\n\nFurther arguments to be passed to RankProfile."
  },
  {
    "objectID": "index.html#motivation",
    "href": "index.html#motivation",
    "title": "Vespa for Data Scientists",
    "section": "Motivation",
    "text": "Motivation\nThis library contains application specific code related to data manipulation and analysis of different Vespa use cases. The Vespa python API is used to interact with Vespa applications from python for faster exploration.\nThe main goal of this space is to facilitate prototyping and experimentation for data scientists. Please visit Vespa sample apps for producuction-ready use cases and Vespa docs for in-depth Vespa documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Vespa for Data Scientists",
    "section": "Install",
    "text": "Install\nCode to support and reproduce the usecases documented here can be found in the learntorank library.\nInstall via PyPI:\npip install learntorank"
  },
  {
    "objectID": "index.html#development",
    "href": "index.html#development",
    "title": "Vespa for Data Scientists",
    "section": "Development",
    "text": "Development\nAll the code and content of this repo is created using nbdev by editting notebooks. We will give a summary below about the main points required to contribute, but we suggest going through nbdev tutorials to learn more.\n\nSetting up environment\n\nCreate and activate a virtual environment of your choice. We recommend pipenv.\npipenv shell\nInstall Jupyter Lab (or Jupyter Notebook if you prefer).\npip3 install jupyterlab\nCreate a new kernel for Jupyter that uses the virtual environment created at step 1.\n\nCheck where the current list of kernels is located with jupyter kernelspec list.\nCopy one of the existing folder and rename it to learntorank.\nModify the kernel.json file that is inside the new folder to reflect the python3executable associated with your virtual env.\n\nInstall nbdev library:\npip3 install nbdev\nInstall learntorank in development mode:\npip3 install -e .[dev]\n\n\n\nMost used nbdev commands\nFrom your terminal:\n\nnbdev_help: List all nbdev commands available.\nnbdev_readme: Update README.md based on index.ipynb\nPreview documentation while editing the notebooks:\n\nnbdev_preview --port 3000\n\nWorkflow before pushing code:\n\nnbdev_test --n_workers 2: Execute all the tests inside notebooks.\n\nTests can run in parallel but since we create Docker containers we suggest a low number of workers to preserve memory.\n\nnbdev_export: Export code from notebooks to the python library.\nnbdev_clean: Clean notebooks to avoid merge conflicts.\n\nPublish library\n\nnbdev_bump_version: Bump library version.\nnbdev_pypi: Publish library to PyPI."
  },
  {
    "objectID": "module_evaluation.html",
    "href": "module_evaluation.html",
    "title": "evaluation",
    "section": "",
    "text": "Abstract and concrete classes related to evaluation metrics.\n\nsource\n\n\n\n EvalMetric ()\n\nAbstract class for evaluation metric.\n\nsource\n\n\n\n\n EvalMetric.evaluate_query (query_results, relevant_docs, id_field,\n                            default_score, detailed_metrics=False)\n\nAbstract method to be implemented by metrics inheriting from EvalMetric to evaluate query results.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nquery_results\n\n\nRaw query results returned by Vespa.\n\n\nrelevant_docs\n\n\nEach dict contains a doc id a optionally a doc score.\n\n\nid_field\n\n\nThe Vespa field representing the document id.\n\n\ndefault_score\n\n\nScore to assign to the additional documents that are not relevant. Default to 0.\n\n\ndetailed_metrics\nbool\nFalse\nReturn intermediate computations if available.\n\n\nReturns\ntyping.Dict\n\nMetric values.\n\n\n\n\nsource\n\n\n\n\n MatchRatio ()\n\nComputes the ratio of documents retrieved by the match phase.\nInstantiate the metric:\n\nmetric = MatchRatio()\n\n\nsource\n\n\n\n\n MatchRatio.evaluate_query (query_results:vespa.io.VespaQueryResponse,\n                            relevant_docs:List[Dict], id_field:str,\n                            default_score:int, detailed_metrics=False)\n\nEvaluate query results according to match ratio metric.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nquery_results\nVespaQueryResponse\n\nRaw query results returned by Vespa.\n\n\nrelevant_docs\ntyping.List[typing.Dict]\n\nEach dict contains a doc id a optionally a doc score.\n\n\nid_field\nstr\n\nThe Vespa field representing the document id.\n\n\ndefault_score\nint\n\nScore to assign to the additional documents that are not relevant. Default to 0.\n\n\ndetailed_metrics\nbool\nFalse\nReturn intermediate computations if available.\n\n\nReturns\ntyping.Dict\n\nReturns the match ratio. In addition, if detailed_metrics=False, returns the number of retrieved docs _retrieved_docs and the number of docs available in the corpus _docs_available.\n\n\n\nCompute match ratio:\n\nevaluation = metric.evaluate_query(\n    query_results=query_results, \n    relevant_docs=None,\n    id_field=\"vespa_id_field\",\n    default_score=0,\n)\nevaluation\n\n{'match_ratio': 0.01731996353691887}\n\n\nReturn detailed metrics, in addition to match ratio:\n\nevaluation = metric.evaluate_query(\n    query_results=query_results,\n    relevant_docs=None,\n    id_field=\"vespa_id_field\",\n    default_score=0,\n    detailed_metrics=True,\n)\nevaluation\n\n{'match_ratio': 0.01731996353691887,\n 'match_ratio_retrieved_docs': 1083,\n 'match_ratio_docs_available': 62529}\n\n\n\nsource\n\n\n\n\n TimeQuery ()\n\nCompute the time it takes for Vespa to execute the query..\nInstantiate the metric:\n\ntime_metric = TimeQuery()\n\n\nsource\n\n\n\n\n TimeQuery.evaluate_query (query_results:vespa.io.VespaQueryResponse,\n                           relevant_docs:List[Dict], id_field:str,\n                           default_score:int, detailed_metrics=False)\n\nEvaluate query results according to query time metric.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nquery_results\nVespaQueryResponse\n\nRaw query results returned by Vespa.\n\n\nrelevant_docs\ntyping.List[typing.Dict]\n\nEach dict contains a doc id a optionally a doc score.\n\n\nid_field\nstr\n\nThe Vespa field representing the document id.\n\n\ndefault_score\nint\n\nScore to assign to the additional documents that are not relevant. Default to 0.\n\n\ndetailed_metrics\nbool\nFalse\nReturn intermediate computations if available.\n\n\nReturns\ntyping.Dict\n\nReturns the match ratio. In addition, if detailed_metrics=False, returns the number of retrieved docs _retrieved_docs and the number of docs available in the corpus _docs_available.\n\n\n\nCompute the query time a client would observe (except network latency).\n\ntime_metric.evaluate_query(\n    query_results=query_results, \n    relevant_docs=None, \n    id_field=\"vespa_id_field\",\n    default_score=0\n)\n\n{'search_time': 0.013}\n\n\nInclude detailed metrics. In addition to the search_time above, it returns the time to execute the first protocol phase/matching phase (search_time_query_time) and the time to execute the summary fill protocol phase for the globally ordered top-k hits (search_time_summary_fetch_time).\n\ntime_metric.evaluate_query(\n    query_results=query_results, \n    relevant_docs=None, \n    id_field=\"vespa_id_field\",\n    default_score=0,\n    detailed_metrics=True\n)\n\n{'search_time': 0.013,\n 'search_time_query_time': 0.01,\n 'search_time_summary_fetch_time': 0.002}\n\n\n\nsource\n\n\n\n\n Recall (at:int)\n\nCompute the recall at position at.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nat\nint\nMaximum position on the resulting list to look for relevant docs.\n\n\nReturns\nNone\n\n\n\n\nInstantiate the metric:\n\nrecall_1 = Recall(at=1)\nrecall_2 = Recall(at=2)\nrecall_3 = Recall(at=3)\n\n\nsource\n\n\n\n\n Recall.evaluate_query (query_results:vespa.io.VespaQueryResponse,\n                        relevant_docs:List[Dict], id_field:str,\n                        default_score:int, detailed_metrics=False)\n\nEvaluate query results according to recall metric.\nThere is an assumption that only documents with score > 0 are relevant. Recall is equal to zero in case no relevant documents with score > 0 is provided.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nquery_results\nVespaQueryResponse\n\nRaw query results returned by Vespa.\n\n\nrelevant_docs\ntyping.List[typing.Dict]\n\nEach dict contains a doc id a optionally a doc score.\n\n\nid_field\nstr\n\nThe Vespa field representing the document id.\n\n\ndefault_score\nint\n\nScore to assign to the additional documents that are not relevant. Default to 0.\n\n\ndetailed_metrics\nbool\nFalse\nReturn intermediate computations if available.\n\n\nReturns\ntyping.Dict\n\nReturns the recall value.\n\n\n\nCompute recall:\n\nevaluation = recall_2.evaluate_query(\n    query_results=query_results,\n    relevant_docs=relevant_docs,\n    id_field=\"vespa_id_field\",\n    default_score=0,\n)\nevaluation\n\n{'recall_2': 0.5}\n\n\nCompute recall:\n\nsource\n\n\n\n\n ReciprocalRank (at:int)\n\nCompute the reciprocal rank at position at\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nat\nint\nMaximum position on the resulting list to look for relevant docs.\n\n\n\nInstantiate the metric:\n\nrr_1 = ReciprocalRank(at=1)\nrr_2 = ReciprocalRank(at=2)\nrr_3 = ReciprocalRank(at=3)\n\n\nsource\n\n\n\n\n ReciprocalRank.evaluate_query (query_results:vespa.io.VespaQueryResponse,\n                                relevant_docs:List[Dict], id_field:str,\n                                default_score:int, detailed_metrics=False)\n\nEvaluate query results according to reciprocal rank metric.\nThere is an assumption that only documents with score > 0 are relevant.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nquery_results\nVespaQueryResponse\n\nRaw query results returned by Vespa.\n\n\nrelevant_docs\ntyping.List[typing.Dict]\n\nEach dict contains a doc id a optionally a doc score.\n\n\nid_field\nstr\n\nThe Vespa field representing the document id.\n\n\ndefault_score\nint\n\nScore to assign to the additional documents that are not relevant. Default to 0.\n\n\ndetailed_metrics\nbool\nFalse\nReturn intermediate computations if available.\n\n\nReturns\ntyping.Dict\n\nReturns the reciprocal rank value.\n\n\n\nCompute reciprocal rank:\n\nevaluation = rr_2.evaluate_query(\n    query_results=query_results,\n    relevant_docs=relevant_docs,\n    id_field=\"vespa_id_field\",\n    default_score=0,\n)\nevaluation\n\n{'reciprocal_rank_2': 0.5}\n\n\n\nsource\n\n\n\n\n NormalizedDiscountedCumulativeGain (at:int)\n\nCompute the normalized discounted cumulative gain at position at.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nat\nint\nMaximum position on the resulting list to look for relevant docs.\n\n\n\nInstantiate the metric:\n\nndcg_1 = NormalizedDiscountedCumulativeGain(at=1)\nndcg_2 = NormalizedDiscountedCumulativeGain(at=2)\nndcg_3 = NormalizedDiscountedCumulativeGain(at=3)\n\n\nsource\n\n\n\n\n NormalizedDiscountedCumulativeGain.evaluate_query\n                                                    (query_results:vespa.i\n                                                    o.VespaQueryResponse, \n                                                    relevant_docs:List[Dic\n                                                    t], id_field:str,\n                                                    default_score:int, det\n                                                    ailed_metrics=False)\n\nEvaluate query results according to normalized discounted cumulative gain.\nThere is an assumption that documents returned by the query that are not included in the set of relevant documents have score equal to zero. Similarly, if the query returns a number N < at documents, we will assume that those N - at missing scores are equal to zero.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nquery_results\nVespaQueryResponse\n\nRaw query results returned by Vespa.\n\n\nrelevant_docs\ntyping.List[typing.Dict]\n\nEach dict contains a doc id a optionally a doc score.\n\n\nid_field\nstr\n\nThe Vespa field representing the document id.\n\n\ndefault_score\nint\n\nScore to assign to the additional documents that are not relevant. Default to 0.\n\n\ndetailed_metrics\nbool\nFalse\nReturn intermediate computations if available.\n\n\nReturns\ntyping.Dict\n\nReturns the normalized discounted cumulative gain. In addition, if detailed_metrics=False, returns the ideal discounted cumulative gain _ideal_dcg, the discounted cumulative gain _dcg.\n\n\n\nCompute NDCG:\n\nmetric = NormalizedDiscountedCumulativeGain(at=2)\nevaluation = ndcg_2.evaluate_query(\n    query_results=query_results,\n    relevant_docs=relevant_docs,\n    id_field=\"vespa_id_field\",\n    default_score=0,\n)\nevaluation\n\n{'ndcg_2': 0.38685280723454163}\n\n\nReturn detailed metrics, in addition to NDCG:\n\nevaluation = ndcg_2.evaluate_query(\n    query_results=query_results,\n    relevant_docs=relevant_docs,\n    id_field=\"vespa_id_field\",\n    default_score=0,\n    detailed_metrics=True,\n)\nevaluation\n\n{'ndcg_2': 0.38685280723454163,\n 'ndcg_2_ideal_dcg': 1.6309297535714575,\n 'ndcg_2_dcg': 0.6309297535714575}"
  },
  {
    "objectID": "module_evaluation.html#evaluation-queries-in-batch",
    "href": "module_evaluation.html#evaluation-queries-in-batch",
    "title": "evaluation",
    "section": "Evaluation queries in batch",
    "text": "Evaluation queries in batch\n\nsource\n\nevaluate\n\n evaluate (app:vespa.application.Vespa,\n           labeled_data:Union[List[Dict],pandas.core.frame.DataFrame],\n           eval_metrics:List[__main__.EvalMetric], query_model:Union[learn\n           torank.query.QueryModel,List[learntorank.query.QueryModel]],\n           id_field:str, default_score:int=0, detailed_metrics=False,\n           per_query=False, aggregators=None, timeout=1000, **kwargs)\n\nEvaluate a QueryModel according to a list of EvalMetric.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\napp\nVespa\n\nConnection to a Vespa application.\n\n\nlabeled_data\ntyping.Union[typing.List[typing.Dict], pandas.core.frame.DataFrame]\n\nData containing query, query_id and relevant docs. See examples below for format.\n\n\neval_metrics\ntyping.List[main.EvalMetric]\n\nEvaluation metrics\n\n\nquery_model\ntyping.Union[learntorank.query.QueryModel, typing.List[learntorank.query.QueryModel]]\n\nQuery models to be evaluated\n\n\nid_field\nstr\n\nThe Vespa field representing the document id.\n\n\ndefault_score\nint\n0\nScore to assign to the additional documents that are not relevant.\n\n\ndetailed_metrics\nbool\nFalse\nReturn intermediate computations if available.\n\n\nper_query\nbool\nFalse\nSet to True to return evaluation metrics per query.\n\n\naggregators\nNoneType\nNone\nUsed only if per_query=False. List of pandas friendly aggregators to summarize per model metrics. We use [“mean”, “median”, “std”] by default.\n\n\ntimeout\nint\n1000\nVespa query timeout in ms.\n\n\nkwargs\n\n\n\n\n\nReturns\nDataFrame\n\nReturns query_id and metrics according to the selected evaluation metrics.\n\n\n\nUsage:\nSetup and feed a Vespa application:\n\nfrom learntorank.passage import create_basic_search_package\nfrom learntorank.passage import PassageData\nfrom vespa.deployment import VespaDocker\n\n\napp_package = create_basic_search_package(name=\"EvaluationApp\")\nvespa_docker = VespaDocker(port=8082, cfgsrv_port=19072)\napp = vespa_docker.deploy(application_package=app_package)\ndata = PassageData.load()\nresponses = app.feed_df(\n    df=data.get_corpus(), \n    include_id=True, \n    id_field=\"doc_id\"\n)\n\nDefine query models to be evaluated:\n\nfrom learntorank.query import OR, Ranking\n\n\nbm25_query_model = QueryModel(\n    name=\"bm25\", \n    match_phase=OR(), \n    ranking=Ranking(name=\"bm25\")\n)\nnative_query_model = QueryModel(\n    name=\"native_rank\", \n    match_phase=OR(), \n    ranking=Ranking(name=\"native_rank\")\n)\n\nDefine metrics to compute during evaluation:\n\nmetrics = [\n    Recall(at=10), \n    ReciprocalRank(at=3), \n    NormalizedDiscountedCumulativeGain(at=3)\n]\n\nGet labeled data:\n\nlabeled_data = data.get_labels(type=\"dev\")\nlabeled_data[0:2]\n\n[{'query_id': '1101971',\n  'query': 'why say the sky is the limit',\n  'relevant_docs': [{'id': '7407715', 'score': 1}]},\n {'query_id': '712898',\n  'query': 'what is an cvc in radiology',\n  'relevant_docs': [{'id': '7661336', 'score': 1}]}]\n\n\nEvaluate:\n\nevaluation = evaluate(\n    app=app,\n    labeled_data=labeled_data, \n    eval_metrics=metrics, \n    query_model=[native_query_model, bm25_query_model], \n    id_field=\"doc_id\",\n)\nevaluation\n\n\n\n\n\n  \n    \n      \n      model\n      bm25\n      native_rank\n    \n  \n  \n    \n      recall_10\n      mean\n      0.935833\n      0.845833\n    \n    \n      median\n      1.000000\n      1.000000\n    \n    \n      std\n      0.215444\n      0.342749\n    \n    \n      reciprocal_rank_3\n      mean\n      0.935000\n      0.746667\n    \n    \n      median\n      1.000000\n      1.000000\n    \n    \n      std\n      0.231977\n      0.399551\n    \n    \n      ndcg_3\n      mean\n      0.912839\n      0.740814\n    \n    \n      median\n      1.000000\n      1.000000\n    \n    \n      std\n      0.242272\n      0.387611\n    \n  \n\n\n\n\nThe evaluate function also accepts labeled data as a data frame:\n\nlabeled_df.head()\n\n\n\n\n\n  \n    \n      \n      qid\n      query\n      doc_id\n      relevance\n    \n  \n  \n    \n      0\n      1101971\n      why say the sky is the limit\n      7407715\n      1\n    \n    \n      1\n      712898\n      what is an cvc in radiology\n      7661336\n      1\n    \n    \n      2\n      154469\n      dmv california how long does it take to get id\n      7914544\n      1\n    \n    \n      3\n      930015\n      what's an epigraph\n      7928705\n      1\n    \n    \n      4\n      860085\n      what is va tax\n      2915383\n      1\n    \n  \n\n\n\n\n\nevaluation_df = evaluate(\n    app=app,\n    labeled_data=labeled_df, \n    eval_metrics=metrics, \n    query_model=[native_query_model, bm25_query_model], \n    id_field=\"doc_id\",\n)\nevaluation_df\n\n\n\n\n\n  \n    \n      \n      model\n      bm25\n      native_rank\n    \n  \n  \n    \n      recall_10\n      mean\n      0.935833\n      0.845833\n    \n    \n      median\n      1.000000\n      1.000000\n    \n    \n      std\n      0.215444\n      0.342749\n    \n    \n      reciprocal_rank_3\n      mean\n      0.935000\n      0.746667\n    \n    \n      median\n      1.000000\n      1.000000\n    \n    \n      std\n      0.231977\n      0.399551\n    \n    \n      ndcg_3\n      mean\n      0.912839\n      0.740814\n    \n    \n      median\n      1.000000\n      1.000000\n    \n    \n      std\n      0.242272\n      0.387611\n    \n  \n\n\n\n\nControl which aggregators are computed:\n\nevaluation = evaluate(\n    app=app,\n    labeled_data=labeled_data, \n    eval_metrics=metrics, \n    query_model=[native_query_model, bm25_query_model], \n    id_field=\"doc_id\",\n    aggregators=[\"mean\", \"std\"]\n)\nevaluation\n\n\n\n\n\n  \n    \n      \n      model\n      bm25\n      native_rank\n    \n  \n  \n    \n      recall_10\n      mean\n      0.935833\n      0.845833\n    \n    \n      std\n      0.215444\n      0.342749\n    \n    \n      reciprocal_rank_3\n      mean\n      0.935000\n      0.746667\n    \n    \n      std\n      0.231977\n      0.399551\n    \n    \n      ndcg_3\n      mean\n      0.912839\n      0.740814\n    \n    \n      std\n      0.242272\n      0.387611\n    \n  \n\n\n\n\nInclude detailed metrics when available, this includes intermediate steps that are available for some of the metrics:\n\nevaluation = evaluate(\n    app=app,\n    labeled_data=labeled_data, \n    eval_metrics=metrics, \n    query_model=[native_query_model, bm25_query_model], \n    id_field=\"doc_id\",\n    aggregators=[\"mean\", \"std\"],\n    detailed_metrics=True\n)\nevaluation\n\n\n\n\n\n  \n    \n      \n      model\n      bm25\n      native_rank\n    \n  \n  \n    \n      recall_10\n      mean\n      0.935833\n      0.845833\n    \n    \n      std\n      0.215444\n      0.342749\n    \n    \n      reciprocal_rank_3\n      mean\n      0.935000\n      0.746667\n    \n    \n      std\n      0.231977\n      0.399551\n    \n    \n      ndcg_3\n      mean\n      0.912839\n      0.740814\n    \n    \n      std\n      0.242272\n      0.387611\n    \n    \n      ndcg_3_ideal_dcg\n      mean\n      1.054165\n      1.054165\n    \n    \n      std\n      0.207315\n      0.207315\n    \n    \n      ndcg_3_dcg\n      mean\n      0.938928\n      0.765474\n    \n    \n      std\n      0.225533\n      0.387161\n    \n  \n\n\n\n\nGenerate results per query:\n\nevaluation = evaluate(\n    app=app,\n    labeled_data=labeled_data, \n    eval_metrics=metrics, \n    query_model=[native_query_model, bm25_query_model], \n    id_field=\"doc_id\",\n    per_query=True\n)\nevaluation.head()\n\n\n\n\n\n  \n    \n      \n      model\n      query_id\n      recall_10\n      reciprocal_rank_3\n      ndcg_3\n    \n  \n  \n    \n      0\n      native_rank\n      1101971\n      1.0\n      1.0\n      1.0\n    \n    \n      1\n      native_rank\n      712898\n      0.0\n      0.0\n      0.0\n    \n    \n      2\n      native_rank\n      154469\n      1.0\n      0.0\n      0.0\n    \n    \n      3\n      native_rank\n      930015\n      1.0\n      0.0\n      0.0\n    \n    \n      4\n      native_rank\n      860085\n      0.0\n      0.0\n      0.0"
  },
  {
    "objectID": "module_evaluation.html#evaluate-specific-query",
    "href": "module_evaluation.html#evaluate-specific-query",
    "title": "evaluation",
    "section": "Evaluate specific query",
    "text": "Evaluate specific query\n\nsource\n\nevaluate_query\n\n evaluate_query (app:vespa.application.Vespa,\n                 eval_metrics:List[__main__.EvalMetric],\n                 query_model:learntorank.query.QueryModel, query_id:str,\n                 query:str, id_field:str, relevant_docs:List[Dict],\n                 default_score:int=0, detailed_metrics=False, **kwargs)\n\nEvaluate a single query according to evaluation metrics\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\napp\nVespa\n\nConnection to a Vespa application.\n\n\neval_metrics\ntyping.List[main.EvalMetric]\n\nEvaluation metrics\n\n\nquery_model\nQueryModel\n\nQuery model to be evaluated\n\n\nquery_id\nstr\n\nQuery id represented as str.\n\n\nquery\nstr\n\nQuery string.\n\n\nid_field\nstr\n\nThe Vespa field representing the document id.\n\n\nrelevant_docs\ntyping.List[typing.Dict]\n\nEach dict contains a doc id a optionally a doc score.\n\n\ndefault_score\nint\n0\nScore to assign to the additional documents that are not relevant.\n\n\ndetailed_metrics\nbool\nFalse\nReturn intermediate computations if available.\n\n\nkwargs\n\n\n\n\n\nReturns\ntyping.Dict\n\nContains query_id and metrics according to the selected evaluation metrics.\n\n\n\nUsage:\n\napp = Vespa(url = \"https://api.cord19.vespa.ai\")\nquery_model = QueryModel(\n    match_phase = OR(),\n    ranking = Ranking(name=\"bm25\", list_features=True))\n\nEvaluate a single query:\n\nquery_evaluation = evaluate_query(\n    app=app,\n    eval_metrics = eval_metrics, \n    query_model = bm25_query_model, \n    query_id = \"0\", \n    query = \"Intrauterine virus infections and congenital heart disease\", \n    id_field = \"id\",\n    relevant_docs = [{\"id\": 0, \"score\": 1}, {\"id\": 3, \"score\": 1}],\n    default_score = 0\n)\nquery_evaluation\n\n{'model': 'bm25',\n 'query_id': '0',\n 'match_ratio': 0.814424921006077,\n 'recall_10': 0.0,\n 'reciprocal_rank_10': 0}"
  },
  {
    "objectID": "module_evaluation.html#evaluate-query-under-specific-document-ids",
    "href": "module_evaluation.html#evaluate-query-under-specific-document-ids",
    "title": "evaluation",
    "section": "Evaluate query under specific document ids",
    "text": "Evaluate query under specific document ids\nUse recall to specify which documents should be included in the evaluation.\nIn the example below, we include documents with id equal to 0, 1 and 2. Since the relevant documents for this query are the documents with id 0 and 3, we should get recall equal to 0.5.\n\nquery_evaluation = evaluate_query(\n    app=app,\n    eval_metrics = eval_metrics, \n    query_model = query_model, \n    query_id = 0, \n    query = \"Intrauterine virus infections and congenital heart disease\", \n    id_field = \"id\",\n    relevant_docs = [{\"id\": 0, \"score\": 1}, {\"id\": 3, \"score\": 1}],\n    default_score = 0,\n    recall = (\"id\", [0, 1, 2])\n)\nquery_evaluation\n\n{'model': 'default_name',\n 'query_id': 0,\n 'match_ratio': 9.70242657688688e-06,\n 'recall_10': 0.5,\n 'reciprocal_rank_10': 1.0}\n\n\nWe now include documents with id equal to 0, 1, 2 and 3. This should give a recall equal to 1.\n\nquery_evaluation = evaluate_query(\n    app=app,\n    eval_metrics = eval_metrics, \n    query_model = query_model, \n    query_id = 0, \n    query = \"Intrauterine virus infections and congenital heart disease\", \n    id_field = \"id\",\n    relevant_docs = [{\"id\": 0, \"score\": 1}, {\"id\": 3, \"score\": 1}],\n    default_score = 0,\n    recall = (\"id\", [0, 1, 2, 3])\n)\nquery_evaluation\n\n{'model': 'default_name',\n 'query_id': 0,\n 'match_ratio': 1.2936568769182506e-05,\n 'recall_10': 1.0,\n 'reciprocal_rank_10': 1.0}"
  },
  {
    "objectID": "passage_dataset.html",
    "href": "passage_dataset.html",
    "title": "Dataset",
    "section": "",
    "text": "For the passage ranking use case, we will use the MS MARCO passage dataset 1 through the ir_datasets library. Besides being convenient, ir_datasets solves encoding errors in the original dataset source files."
  },
  {
    "objectID": "passage_dataset.html#data-exploration",
    "href": "passage_dataset.html#data-exploration",
    "title": "Dataset",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nDocument corpus\nStart by loading the data. The dataset will be downloaded once and cached on disk for future use, so it takes a while the first time the command below is run.\n\npassage_corpus = ir_datasets.load(\"msmarco-passage\")\n\nNumber of passages in the document corpus:\n\npassage_corpus.docs_count()\n\n8841823\n\n\nSample a few passages of the document corpus.\n\npd.DataFrame(passage_corpus.docs_iter()[0:5])\n\n\n\n\n\n  \n    \n      \n      doc_id\n      text\n    \n  \n  \n    \n      0\n      0\n      The presence of communication amid scientific ...\n    \n    \n      1\n      1\n      The Manhattan Project and its atomic bomb help...\n    \n    \n      2\n      2\n      Essay on The Manhattan Project - The Manhattan...\n    \n    \n      3\n      3\n      The Manhattan Project was the name for a proje...\n    \n    \n      4\n      4\n      versions of each volume as well as complementa...\n    \n  \n\n\n\n\n\n\nTraining data\nLoad the training data. We use the judged version that only include queries with at least one relevance judgement.\n\npassage_train = ir_datasets.load(\"msmarco-passage/train/judged\")\n\n\nRelevant documents\nNumber of relevant judgements:\n\npassage_train.qrels_count()\n\n532761\n\n\nFor each query id, there is a dict of relevant documents containing the document id as key and the relevance score as value.\n\nfrom learntorank.passage import sample_dict_items\n\ntrain_qrels_dict = passage_train.qrels_dict()\nsample_dict_items(train_qrels_dict, 5)\n\n{'1038069': {'2293922': 1},\n '700425': {'4351261': 1},\n '926242': {'3500124': 1},\n '690553': {'2877918': 1},\n '411317': {'2230220': 1}}\n\n\nIt is interesting to check what is the range of values of the relevance score. The code below shows that the only score available is 1, indicating that the particular document id is relevant to the query id.\n\nset([score \n     for relevant in train_qrels_dict.values() \n     for score in relevant.values()]\n   )\n\n{1}\n\n\n\n\nQueries\nNumber of training queries:\n\npassage_train.queries_count()\n\n502939\n\n\nThe number of queries differs from the number of relevant documents because some of the queries have more than one relevant document associated with it.\nEach query contains a query id and a query text.\n\ntraining_queries = pd.DataFrame(passage_train.queries_iter())\ntraining_queries.head()\n\n\n\n\n\n  \n    \n      \n      query_id\n      text\n    \n  \n  \n    \n      0\n      121352\n      define extreme\n    \n    \n      1\n      634306\n      what does chattel mean on credit history\n    \n    \n      2\n      920825\n      what was the great leap forward brainly\n    \n    \n      3\n      510633\n      tattoo fixers how much does it cost\n    \n    \n      4\n      737889\n      what is decentralization process.\n    \n  \n\n\n\n\n\n\n\nDevelopment data\nSimilarly to the training data, we can load the judged development data and take a look at the queries and relevance judgements.\n\npassage_dev = ir_datasets.load(\"msmarco-passage/dev/judged\")\n\n\nRelevant documents\nNumber of relevant judgements:\n\npassage_dev.qrels_count()\n\n59273\n\n\nFor each query id, there is a dict of relevant documents containing the document id as key and the relevance score as value.\n\ndev_qrels_dict = passage_dev.qrels_dict()\nsample_dict_items(dev_qrels_dict, 5)\n\n{'255': {'7629892': 1},\n '611327': {'7610137': 1},\n '584695': {'7408281': 1},\n '300246': {'7814106': 1, '7814107': 1},\n '739094': {'7640560': 1}}\n\n\n\n\nQueries\nNumber of dev queries:\n\npassage_dev.queries_count()\n\n55578\n\n\nEach query contains a query id and a query text.\n\ndev_queries = pd.DataFrame(passage_dev.queries_iter())\ndev_queries.head()\n\n\n\n\n\n  \n    \n      \n      query_id\n      text\n    \n  \n  \n    \n      0\n      1048578\n      cost of endless pools/swim spa\n    \n    \n      1\n      1048579\n      what is pcnt\n    \n    \n      2\n      1048582\n      what is paysky\n    \n    \n      3\n      1048583\n      what is paydata\n    \n    \n      4\n      1048585\n      what is paula deen's brother"
  },
  {
    "objectID": "passage_dataset.html#data-manipulation",
    "href": "passage_dataset.html#data-manipulation",
    "title": "Dataset",
    "section": "Data Manipulation",
    "text": "Data Manipulation\n\nSample data\nGiven the large amount of data, it is useful to properly sample data when prototyping, which can be done with the sample_data function. This might take same time in case the full dataset needs to be downloaded for the first time.\n\nfrom learntorank.passage import sample_data\n\npassage_sample = sample_data(n_relevant=100, n_irrelevant=800)\n\n\npassage_sample\n\nPassageData(corpus, train_qrels, train_queries, dev_qrels, dev_queries)\n\n\n\nSave\nWe can save the sampled data to disk to avoid regenerating it everytime we need to use it.\n\npassage_sample.save(\"sample.json\")\n\n\n\nLoad\nLoad the data back when needed with PassageData.load:\n\nfrom learntorank.passage import PassageData\n\nloaded_sample = PassageData.load(file_path=\"sample.json\")"
  }
]